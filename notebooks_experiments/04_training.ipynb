{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "from functools import partial\n",
    "from typing import Dict, List, NoReturn\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.plugins import DDPPlugin\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YAML / LOGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def read_yaml(config_yaml: str) -> Dict:\n",
    "    \"\"\"Read config file to dictionary.\n",
    "\n",
    "    Args:\n",
    "        config_yaml: str\n",
    "\n",
    "    Returns:\n",
    "        configs: Dict\n",
    "    \"\"\"\n",
    "    with open(config_yaml, \"r\") as fr:\n",
    "        configs = yaml.load(fr, Loader=yaml.FullLoader)\n",
    "\n",
    "    return configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_configs_gramma(configs: Dict) -> NoReturn:\n",
    "    r\"\"\"Check if the gramma of the config dictionary for training is legal.\"\"\"\n",
    "\n",
    "    paired_input_target_data = configs['train']['paired_input_target_data']\n",
    "\n",
    "    if paired_input_target_data is False:\n",
    "\n",
    "        input_source_types = configs['train']['input_source_types']\n",
    "        augmentation_types = configs['train']['augmentations'].keys()\n",
    "\n",
    "        for augmentation_type in list(\n",
    "            set(augmentation_types)\n",
    "            & set(\n",
    "                [\n",
    "                    'mixaudio',\n",
    "                    'pitch_shift',\n",
    "                    'magnitude_scale',\n",
    "                    'swap_channel',\n",
    "                    'flip_axis',\n",
    "                ]\n",
    "            )\n",
    "        ):\n",
    "\n",
    "            augmentation_dict = configs['train']['augmentations'][augmentation_type]\n",
    "\n",
    "            for source_type in augmentation_dict.keys():\n",
    "                if source_type not in input_source_types:\n",
    "                    error_msg = (\n",
    "                        \"The source type '{}'' in configs['train']['augmentations']['{}'] \"\n",
    "                        \"must be one of input_source_types {}\".format(\n",
    "                            source_type, augmentation_type, input_source_types\n",
    "                        )\n",
    "                    )\n",
    "                    raise Exception(error_msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "def create_logging(log_dir: str, filemode: str) -> logging:\n",
    "    r\"\"\"Create logging to write out log files.\n",
    "\n",
    "    Args:\n",
    "        logs_dir, str, directory to write out logs\n",
    "        filemode: str, e.g., \"w\"\n",
    "\n",
    "    Returns:\n",
    "        logging\n",
    "    \"\"\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    i1 = 0\n",
    "\n",
    "    while os.path.isfile(os.path.join(log_dir, \"{:04d}.log\".format(i1))):\n",
    "        i1 += 1\n",
    "\n",
    "    log_path = os.path.join(log_dir, \"{:04d}.log\".format(i1))\n",
    "    logging.basicConfig(\n",
    "        level=logging.DEBUG,\n",
    "        format=\"%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s\",\n",
    "        datefmt=\"%a, %d %b %Y %H:%M:%S\",\n",
    "        filename=log_path,\n",
    "        filemode=filemode,\n",
    "    )\n",
    "\n",
    "    # Print to console\n",
    "    console = logging.StreamHandler()\n",
    "    console.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\"%(name)-12s: %(levelname)-8s %(message)s\")\n",
    "    console.setFormatter(formatter)\n",
    "    logging.getLogger(\"\").addHandler(console)\n",
    "\n",
    "    return logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dirs(\n",
    "    workspace: str,\n",
    "    task_name: str,\n",
    "    filename: str,\n",
    "    config_yaml: str,\n",
    "    gpus: int,\n",
    ") -> List[str]:\n",
    "    r\"\"\"Get directory paths.\n",
    "\n",
    "    Args:\n",
    "        workspace: str\n",
    "        task_name, str, e.g., 'musdb18'\n",
    "        filenmae: str\n",
    "        config_yaml: str\n",
    "        gpus: int, e.g., 0 for cpu and 8 for training with 8 gpu cards\n",
    "\n",
    "    Returns:\n",
    "        checkpoints_dir: str\n",
    "        logs_dir: str\n",
    "        logger: pl.loggers.TensorBoardLogger\n",
    "        statistics_path: str\n",
    "    \"\"\"\n",
    "\n",
    "    # save checkpoints dir\n",
    "    checkpoints_dir = os.path.join(\n",
    "        workspace,\n",
    "        \"checkpoints\",\n",
    "        task_name,\n",
    "        filename,\n",
    "        \"config={},gpus={}\".format(pathlib.Path(config_yaml).stem, gpus),\n",
    "    )\n",
    "    os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "\n",
    "    # logs dir\n",
    "    logs_dir = os.path.join(\n",
    "        workspace,\n",
    "        \"logs\",\n",
    "        task_name,\n",
    "        filename,\n",
    "        \"config={},gpus={}\".format(pathlib.Path(config_yaml).stem, gpus),\n",
    "    )\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "    # loggings\n",
    "    create_logging(logs_dir, filemode='w')\n",
    "    # logging.info(args)\n",
    "\n",
    "    # tensorboard logs dir\n",
    "    tb_logs_dir = os.path.join(workspace, \"tensorboard_logs\")\n",
    "    os.makedirs(tb_logs_dir, exist_ok=True)\n",
    "\n",
    "    experiment_name = os.path.join(task_name, filename, pathlib.Path(config_yaml).stem)\n",
    "    logger = pl.loggers.TensorBoardLogger(save_dir=tb_logs_dir, name=experiment_name)\n",
    "\n",
    "    # statistics path\n",
    "    statistics_path = os.path.join(\n",
    "        workspace,\n",
    "        \"statistics\",\n",
    "        task_name,\n",
    "        filename,\n",
    "        \"config={},gpus={}\".format(pathlib.Path(config_yaml).stem, gpus),\n",
    "        \"statistics.pkl\",\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(statistics_path), exist_ok=True)\n",
    "\n",
    "    return checkpoints_dir, logs_dir, logger, statistics_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "from typing import Dict, NoReturn\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "\n",
    "def create_logging(log_dir: str, filemode: str) -> logging:\n",
    "    r\"\"\"Create logging to write out log files.\n",
    "\n",
    "    Args:\n",
    "        logs_dir, str, directory to write out logs\n",
    "        filemode: str, e.g., \"w\"\n",
    "\n",
    "    Returns:\n",
    "        logging\n",
    "    \"\"\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    i1 = 0\n",
    "\n",
    "    while os.path.isfile(os.path.join(log_dir, \"{:04d}.log\".format(i1))):\n",
    "        i1 += 1\n",
    "\n",
    "    log_path = os.path.join(log_dir, \"{:04d}.log\".format(i1))\n",
    "    logging.basicConfig(\n",
    "        level=logging.DEBUG,\n",
    "        format=\"%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s\",\n",
    "        datefmt=\"%a, %d %b %Y %H:%M:%S\",\n",
    "        filename=log_path,\n",
    "        filemode=filemode,\n",
    "    )\n",
    "\n",
    "    # Print to console\n",
    "    console = logging.StreamHandler()\n",
    "    console.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\"%(name)-12s: %(levelname)-8s %(message)s\")\n",
    "    console.setFormatter(formatter)\n",
    "    logging.getLogger(\"\").addHandler(console)\n",
    "\n",
    "    return logging\n",
    "\n",
    "\n",
    "def load_audio(\n",
    "    audio_path: str,\n",
    "    mono: bool,\n",
    "    sample_rate: float,\n",
    "    offset: float = 0.0,\n",
    "    duration: float = None,\n",
    ") -> np.array:\n",
    "    r\"\"\"Load audio.\n",
    "\n",
    "    Args:\n",
    "        audio_path: str\n",
    "        mono: bool\n",
    "        sample_rate: float\n",
    "    \"\"\"\n",
    "    audio, _ = librosa.core.load(\n",
    "        audio_path, sr=sample_rate, mono=mono, offset=offset, duration=duration\n",
    "    )\n",
    "    # (audio_samples,) | (channels_num, audio_samples)\n",
    "\n",
    "    if audio.ndim == 1:\n",
    "        audio = audio[None, :]\n",
    "        # (1, audio_samples,)\n",
    "\n",
    "    return audio\n",
    "\n",
    "\n",
    "def load_random_segment(\n",
    "    audio_path: str,\n",
    "    random_state: int,\n",
    "    segment_seconds: float,\n",
    "    mono: bool,\n",
    "    sample_rate: int,\n",
    ") -> np.array:\n",
    "    r\"\"\"Randomly select an audio segment from a recording.\"\"\"\n",
    "\n",
    "    duration = librosa.get_duration(filename=audio_path)\n",
    "\n",
    "    start_time = random_state.uniform(0.0, duration - segment_seconds)\n",
    "\n",
    "    audio = load_audio(\n",
    "        audio_path=audio_path,\n",
    "        mono=mono,\n",
    "        sample_rate=sample_rate,\n",
    "        offset=start_time,\n",
    "        duration=segment_seconds,\n",
    "    )\n",
    "    # (channels_num, audio_samples)\n",
    "\n",
    "    return audio\n",
    "\n",
    "\n",
    "def float32_to_int16(x: np.float32) -> np.int16:\n",
    "\n",
    "    x = np.clip(x, a_min=-1, a_max=1)\n",
    "\n",
    "    return (x * 32767.0).astype(np.int16)\n",
    "\n",
    "\n",
    "def int16_to_float32(x: np.int16) -> np.float32:\n",
    "\n",
    "    return (x / 32767.0).astype(np.float32)\n",
    "\n",
    "\n",
    "def read_yaml(config_yaml: str) -> Dict:\n",
    "    \"\"\"Read config file to dictionary.\n",
    "\n",
    "    Args:\n",
    "        config_yaml: str\n",
    "\n",
    "    Returns:\n",
    "        configs: Dict\n",
    "    \"\"\"\n",
    "    with open(config_yaml, \"r\") as fr:\n",
    "        configs = yaml.load(fr, Loader=yaml.FullLoader)\n",
    "\n",
    "    return configs\n",
    "\n",
    "\n",
    "def check_configs_gramma(configs: Dict) -> NoReturn:\n",
    "    r\"\"\"Check if the gramma of the config dictionary for training is legal.\"\"\"\n",
    "\n",
    "    paired_input_target_data = configs['train']['paired_input_target_data']\n",
    "\n",
    "    if paired_input_target_data is False:\n",
    "\n",
    "        input_source_types = configs['train']['input_source_types']\n",
    "        augmentation_types = configs['train']['augmentations'].keys()\n",
    "\n",
    "        for augmentation_type in list(\n",
    "            set(augmentation_types)\n",
    "            & set(\n",
    "                [\n",
    "                    'mixaudio',\n",
    "                    'pitch_shift',\n",
    "                    'magnitude_scale',\n",
    "                    'swap_channel',\n",
    "                    'flip_axis',\n",
    "                ]\n",
    "            )\n",
    "        ):\n",
    "\n",
    "            augmentation_dict = configs['train']['augmentations'][augmentation_type]\n",
    "\n",
    "            for source_type in augmentation_dict.keys():\n",
    "                if source_type not in input_source_types:\n",
    "                    error_msg = (\n",
    "                        \"The source type '{}'' in configs['train']['augmentations']['{}'] \"\n",
    "                        \"must be one of input_source_types {}\".format(\n",
    "                            source_type, augmentation_type, input_source_types\n",
    "                        )\n",
    "                    )\n",
    "                    raise Exception(error_msg)\n",
    "\n",
    "\n",
    "def magnitude_to_db(x: float) -> float:\n",
    "    eps = 1e-10\n",
    "    return 20.0 * np.log10(max(x, eps))\n",
    "\n",
    "\n",
    "def db_to_magnitude(x: float) -> float:\n",
    "    return 10.0 ** (x / 20)\n",
    "\n",
    "\n",
    "def get_pitch_shift_factor(shift_pitch: float) -> float:\n",
    "    r\"\"\"The factor of the audio length to be scaled.\"\"\"\n",
    "    return 2 ** (shift_pitch / 12)\n",
    "\n",
    "\n",
    "class StatisticsContainer(object):\n",
    "    def __init__(self, statistics_path):\n",
    "        self.statistics_path = statistics_path\n",
    "\n",
    "        self.backup_statistics_path = \"{}_{}.pkl\".format(\n",
    "            os.path.splitext(self.statistics_path)[0],\n",
    "            datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"),\n",
    "        )\n",
    "\n",
    "        self.statistics_dict = {\"train\": [], \"test\": []}\n",
    "\n",
    "    def append(self, steps, statistics, split):\n",
    "        statistics[\"steps\"] = steps\n",
    "        self.statistics_dict[split].append(statistics)\n",
    "\n",
    "    def dump(self):\n",
    "        pickle.dump(self.statistics_dict, open(self.statistics_path, \"wb\"))\n",
    "        pickle.dump(self.statistics_dict, open(self.backup_statistics_path, \"wb\"))\n",
    "        logging.info(\"    Dump statistics to {}\".format(self.statistics_path))\n",
    "        logging.info(\"    Dump statistics to {}\".format(self.backup_statistics_path))\n",
    "\n",
    "    '''\n",
    "    def load_state_dict(self, resume_steps):\n",
    "        self.statistics_dict = pickle.load(open(self.statistics_path, \"rb\"))\n",
    "\n",
    "        resume_statistics_dict = {\"train\": [], \"test\": []}\n",
    "\n",
    "        for key in self.statistics_dict.keys():\n",
    "            for statistics in self.statistics_dict[key]:\n",
    "                if statistics[\"steps\"] <= resume_steps:\n",
    "                    resume_statistics_dict[key].append(statistics)\n",
    "\n",
    "        self.statistics_dict = resume_statistics_dict\n",
    "    '''\n",
    "\n",
    "\n",
    "def calculate_sdr(ref: np.array, est: np.array) -> float:\n",
    "    s_true = ref\n",
    "    s_artif = est - ref\n",
    "    sdr = 10.0 * (\n",
    "        np.log10(np.clip(np.mean(s_true ** 2), 1e-8, np.inf))\n",
    "        - np.log10(np.clip(np.mean(s_artif ** 2), 1e-8, np.inf))\n",
    "    )\n",
    "    return sdr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# from bytesep.utils import db_to_magnitude, get_pitch_shift_factor, magnitude_to_db\n",
    "\n",
    "\n",
    "class Augmentor:\n",
    "    def __init__(self, augmentations: Dict, random_seed=1234):\n",
    "        r\"\"\"Augmentor for augmenting one segment.\n",
    "\n",
    "        Args:\n",
    "            augmentations: Dict, e.g, {\n",
    "                'mixaudio': {'vocals': 2, 'accompaniment': 2}\n",
    "                'pitch_shift': {'vocals': 4, 'accompaniment': 4},\n",
    "                ...,\n",
    "            }\n",
    "            random_seed: int\n",
    "        \"\"\"\n",
    "        self.augmentations = augmentations\n",
    "        self.random_state = np.random.RandomState(random_seed)\n",
    "\n",
    "    def __call__(self, waveform: np.array, source_type: str) -> np.array:\n",
    "        r\"\"\"Augment a waveform.\n",
    "\n",
    "        Args:\n",
    "            waveform: (input_channels, audio_samples)\n",
    "            source_type: str\n",
    "\n",
    "        Returns:\n",
    "            new_waveform: (input_channels, new_audio_samples)\n",
    "        \"\"\"\n",
    "        if 'pitch_shift' in self.augmentations.keys():\n",
    "            waveform = self.pitch_shift(waveform, source_type)\n",
    "\n",
    "        if 'magnitude_scale' in self.augmentations.keys():\n",
    "            waveform = self.magnitude_scale(waveform, source_type)\n",
    "\n",
    "        if 'swap_channel' in self.augmentations.keys():\n",
    "            waveform = self.swap_channel(waveform, source_type)\n",
    "\n",
    "        if 'flip_axis' in self.augmentations.keys():\n",
    "            waveform = self.flip_axis(waveform, source_type)\n",
    "\n",
    "        return waveform\n",
    "\n",
    "    def pitch_shift(self, waveform: np.array, source_type: str) -> np.array:\n",
    "        r\"\"\"Shift the pitch of a waveform. We use resampling for fast pitch\n",
    "        shifting, so the speed of the waveform will also be changed. The length\n",
    "        of the returned waveform will be changed.\n",
    "\n",
    "        Args:\n",
    "            waveform: (input_channels, audio_samples)\n",
    "            source_type: str\n",
    "\n",
    "        Returns:\n",
    "            new_waveform: (input_channels, new_audio_samples)\n",
    "        \"\"\"\n",
    "\n",
    "        # maximum pitch shift in semitones\n",
    "        max_pitch_shift = self.augmentations['pitch_shift'][source_type]\n",
    "\n",
    "        if max_pitch_shift == 0:  # No pitch shift augmentations.\n",
    "            return waveform\n",
    "\n",
    "        # random pitch shift\n",
    "        rand_pitch = self.random_state.uniform(\n",
    "            low=-max_pitch_shift, high=max_pitch_shift\n",
    "        )\n",
    "\n",
    "        # We use librosa.resample instead of librosa.effects.pitch_shift\n",
    "        # because it is 10x times faster.\n",
    "        pitch_shift_factor = get_pitch_shift_factor(rand_pitch)\n",
    "        dummy_sample_rate = 10000  # Dummy constant.\n",
    "\n",
    "        input_channels = waveform.shape[0]\n",
    "\n",
    "        if input_channels == 1:\n",
    "            waveform = np.squeeze(waveform)\n",
    "\n",
    "        new_waveform = librosa.resample(\n",
    "            y=waveform,\n",
    "            orig_sr=dummy_sample_rate,\n",
    "            target_sr=dummy_sample_rate / pitch_shift_factor,\n",
    "            res_type='linear',\n",
    "            axis=-1,\n",
    "        )\n",
    "\n",
    "        if input_channels == 1:\n",
    "            new_waveform = new_waveform[None, :]\n",
    "\n",
    "        return new_waveform\n",
    "\n",
    "    def magnitude_scale(self, waveform: np.array, source_type: str) -> np.array:\n",
    "        r\"\"\"Scale the magnitude of a waveform.\n",
    "\n",
    "        Args:\n",
    "            waveform: (input_channels, audio_samples)\n",
    "            source_type: str\n",
    "\n",
    "        Returns:\n",
    "            new_waveform: (input_channels, audio_samples)\n",
    "        \"\"\"\n",
    "        lower_db = self.augmentations['magnitude_scale'][source_type]['lower_db']\n",
    "        higher_db = self.augmentations['magnitude_scale'][source_type]['higher_db']\n",
    "\n",
    "        if lower_db == 0 and higher_db == 0:  # No magnitude scale augmentation.\n",
    "            return waveform\n",
    "\n",
    "        # The magnitude (in dB) of the sample with the maximum value.\n",
    "        waveform_db = magnitude_to_db(np.max(np.abs(waveform)))\n",
    "\n",
    "        new_waveform_db = self.random_state.uniform(\n",
    "            waveform_db + lower_db, waveform_db + higher_db\n",
    "        )\n",
    "\n",
    "        relative_db = new_waveform_db - waveform_db\n",
    "\n",
    "        relative_scale = db_to_magnitude(relative_db)\n",
    "\n",
    "        new_waveform = waveform * relative_scale\n",
    "\n",
    "        return new_waveform\n",
    "\n",
    "    def swap_channel(self, waveform: np.array, source_type: str) -> np.array:\n",
    "        r\"\"\"Randomly swap channels.\n",
    "\n",
    "        Args:\n",
    "            waveform: (input_channels, audio_samples)\n",
    "            source_type: str\n",
    "\n",
    "        Returns:\n",
    "            new_waveform: (input_channels, audio_samples)\n",
    "        \"\"\"\n",
    "        ndim = waveform.shape[0]\n",
    "\n",
    "        if ndim == 1:\n",
    "            return waveform\n",
    "        else:\n",
    "            random_axes = self.random_state.permutation(ndim)\n",
    "            return waveform[random_axes, :]\n",
    "\n",
    "    def flip_axis(self, waveform: np.array, source_type: str) -> np.array:\n",
    "        r\"\"\"Randomly flip the waveform along x-axis.\n",
    "\n",
    "        Args:\n",
    "            waveform: (input_channels, audio_samples)\n",
    "            source_type: str\n",
    "\n",
    "        Returns:\n",
    "            new_waveform: (input_channels, audio_samples)\n",
    "        \"\"\"\n",
    "        ndim = waveform.shape[0]\n",
    "        random_values = self.random_state.choice([-1, 1], size=ndim)\n",
    "\n",
    "        return waveform * random_values[:, None]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from typing import Dict, List, NoReturn\n",
    "\n",
    "import numpy as np\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "class SegmentSampler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        indexes_dict_path: str,\n",
    "        input_source_types: List[str],\n",
    "        target_source_types: List[str],\n",
    "        segment_samples: int,\n",
    "        remixing_sources: bool,\n",
    "        mixaudio_dict: Dict,\n",
    "        batch_size: int,\n",
    "        steps_per_epoch: int,\n",
    "        random_seed=1234,\n",
    "    ):\n",
    "        r\"\"\"Sample training indexes of sources.\n",
    "\n",
    "        Args:\n",
    "            indexes_path: str, path of indexes dict\n",
    "            input_source_types: list of str, e.g., ['vocals', 'accompaniment']\n",
    "            target_source_types: list of str, e.g., ['vocals']\n",
    "            segment_samplers: int\n",
    "            mixaudio_dict, dict, mix-audio data augmentation parameters,\n",
    "                e.g., {'voclas': 2, 'accompaniment': 2}\n",
    "            batch_size: int\n",
    "            steps_per_epoch: int, #steps_per_epoch is called an `epoch`\n",
    "            random_seed: int\n",
    "        \"\"\"\n",
    "        self.segment_samples = segment_samples\n",
    "        self.mixaudio_dict = mixaudio_dict\n",
    "        self.remixing_sources = remixing_sources\n",
    "        self.batch_size = batch_size\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        self.meta_dict = pickle.load(open(indexes_dict_path, \"rb\"))\n",
    "        # E.g., {\n",
    "        #     'vocals': [\n",
    "        #         {'hdf5_path': 'songA.h5', 'key_in_hdf5': 'vocals', 'begin_sample': 0, 'end_sample': 132300}，\n",
    "        #         {'hdf5_path': 'songB.h5', 'key_in_hdf5': 'vocals', 'begin_sample': 4410, 'end_sample': 445410},\n",
    "        #         ... (e.g., 225752 dicts)\n",
    "        #     ],\n",
    "        #     'accompaniment': [\n",
    "        #         {'hdf5_path': 'songA.h5', 'key_in_hdf5': 'vocals', 'begin_sample': 0, 'end_sample': 132300}，\n",
    "        #         {'hdf5_path': 'songB.h5', 'key_in_hdf5': 'vocals', 'begin_sample': 4410, 'end_sample': 445410},\n",
    "        #         ... (e.g., 225752 dicts)\n",
    "        #     ]\n",
    "        # }\n",
    "\n",
    "        self.source_types = list(set(input_source_types) | set(target_source_types))\n",
    "        # E.g., ['vocals', 'accompaniment']\n",
    "\n",
    "        self.pointers_dict = {source_type: 0 for source_type in self.source_types}\n",
    "        # E.g., {'vocals': 0, 'accompaniment': 0}\n",
    "\n",
    "        self.indexes_dict = {\n",
    "            source_type: np.arange(len(self.meta_dict[source_type]))\n",
    "            for source_type in self.source_types\n",
    "        }\n",
    "        # E.g. {\n",
    "        #     'vocals': [0, 1, ..., 225751],\n",
    "        #     'accompaniment': [0, 1, ..., 225751]\n",
    "        # }\n",
    "\n",
    "        random_state = np.random.RandomState(random_seed)\n",
    "        self.random_state_dict = {}\n",
    "\n",
    "        for source_type in self.source_types:\n",
    "\n",
    "            if remixing_sources:\n",
    "                # Use different seeds for different sources.\n",
    "                source_random_seed = random_state.randint(low=0, high=10000)\n",
    "\n",
    "            else:\n",
    "                # Use same seeds for different sources.\n",
    "                source_random_seed = random_seed\n",
    "\n",
    "            self.random_state_dict[source_type] = np.random.RandomState(\n",
    "                source_random_seed\n",
    "            )\n",
    "\n",
    "            self.random_state_dict[source_type].shuffle(self.indexes_dict[source_type])\n",
    "            # E.g., [198036, 196736, ..., 103408]\n",
    "\n",
    "            print(\"{}: {}\".format(source_type, len(self.indexes_dict[source_type])))\n",
    "\n",
    "    def __iter__(self) -> List[Dict]:\n",
    "        r\"\"\"Yield a batch of meta info.\n",
    "\n",
    "        Returns:\n",
    "            batch_meta_list: (batch_size,) e.g., when mix-audio is 2, looks like [\n",
    "                {'vocals': [\n",
    "                    {'hdf5_path': 'songA.h5', 'key_in_hdf5': 'vocals', 'begin_sample': 13406400, 'end_sample': 13538700},\n",
    "                    {'hdf5_path': 'songB.h5', 'key_in_hdf5': 'vocals', 'begin_sample': 4440870, 'end_sample': 4573170}]\n",
    "                'accompaniment': [\n",
    "                    {'hdf5_path': 'songE.h5', 'key_in_hdf5': 'accompaniment', 'begin_sample': 14579460, 'end_sample': 14711760},\n",
    "                    {'hdf5_path': 'songF.h5', 'key_in_hdf5': 'accompaniment', 'begin_sample': 3995460, 'end_sample': 4127760}]\n",
    "                },\n",
    "                ...\n",
    "            ]\n",
    "        \"\"\"\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        while True:\n",
    "            batch_meta_dict = {source_type: [] for source_type in self.source_types}\n",
    "\n",
    "            for source_type in self.source_types:\n",
    "                # E.g., ['vocals', 'accompaniment']\n",
    "\n",
    "                # Loop until get a mini-batch.\n",
    "                while len(batch_meta_dict[source_type]) != batch_size:\n",
    "\n",
    "                    if source_type in self.mixaudio_dict.keys():\n",
    "                        mix_audios_num = self.mixaudio_dict[source_type]\n",
    "\n",
    "                    else:\n",
    "                        mix_audios_num = 1\n",
    "\n",
    "                    largest_index = len(self.indexes_dict[source_type]) - mix_audios_num\n",
    "                    # E.g., 225750 = 225752 - 2\n",
    "\n",
    "                    if self.pointers_dict[source_type] > largest_index:\n",
    "\n",
    "                        # Reset pointer, and shuffle indexes.\n",
    "                        self.pointers_dict[source_type] = 0\n",
    "                        self.random_state_dict[source_type].shuffle(\n",
    "                            self.indexes_dict[source_type]\n",
    "                        )\n",
    "\n",
    "                    source_metas = []\n",
    "\n",
    "                    for _ in range(mix_audios_num):\n",
    "\n",
    "                        pointer = self.pointers_dict[source_type]\n",
    "                        # E.g., 1\n",
    "\n",
    "                        index = self.indexes_dict[source_type][pointer]\n",
    "                        # E.g., 12231\n",
    "\n",
    "                        self.pointers_dict[source_type] += 1\n",
    "\n",
    "                        source_meta = self.meta_dict[source_type][index]\n",
    "                        # E.g., {\n",
    "                        #     'hdf5_path': 'xx/song_A.h5',\n",
    "                        #     'key_in_hdf5': 'vocals',\n",
    "                        #     'begin_sample': 13406400,\n",
    "                        # }\n",
    "\n",
    "                        # Re-assign the end_sample.\n",
    "                        source_meta['end_sample'] = (\n",
    "                            source_meta['begin_sample'] + self.segment_samples\n",
    "                        )\n",
    "\n",
    "                        source_metas.append(source_meta)\n",
    "\n",
    "                    batch_meta_dict[source_type].append(source_metas)\n",
    "\n",
    "            # When mix-audio is 2, batch_meta_dict looks like: {\n",
    "            #     'vocals': [\n",
    "            #         [{'hdf5_path': 'songA.h5', 'key_in_hdf5': 'vocals', 'begin_sample': 13406400, 'end_sample': 13538700},\n",
    "            #          {'hdf5_path': 'songB.h5', 'key_in_hdf5': 'vocals', 'begin_sample': 4440870, 'end_sample': 4573170}\n",
    "            #         ],\n",
    "            #         ... (batch_size)\n",
    "            #     ]\n",
    "            #     'accompaniment': [\n",
    "            #         [{'hdf5_path': 'songG.h5', 'key_in_hdf5': 'vocals', 'begin_sample': 24232950, 'end_sample': 24365250},\n",
    "            #          {'hdf5_path': 'songH.h5', 'key_in_hdf5': 'vocals', 'begin_sample': 1569960, 'end_sample': 1702260}\n",
    "            #         ],\n",
    "            #         ... (batch_size)\n",
    "            #     ]\n",
    "            # }\n",
    "\n",
    "            batch_meta_list = [\n",
    "                {\n",
    "                    source_type: batch_meta_dict[source_type][i]\n",
    "                    for source_type in self.source_types\n",
    "                }\n",
    "                for i in range(batch_size)\n",
    "            ]\n",
    "            # When mix-audio is 2, batch_meta_list looks like: [\n",
    "            #     {'vocals': [\n",
    "            #         {'hdf5_path': 'songA.h5', 'key_in_hdf5': 'vocals', 'begin_sample': 13406400, 'end_sample': 13538700},\n",
    "            #         {'hdf5_path': 'songB.h5', 'key_in_hdf5': 'vocals', 'begin_sample': 4440870, 'end_sample': 4573170}]\n",
    "            #      'accompaniment': [\n",
    "            #         {'hdf5_path': 'songE.h5', 'key_in_hdf5': 'vocals', 'begin_sample': 14579460, 'end_sample': 14711760},\n",
    "            #         {'hdf5_path': 'songF.h5', 'key_in_hdf5': 'vocals', 'begin_sample': 3995460, 'end_sample': 4127760}]\n",
    "            #     }\n",
    "            #     ... (batch_size)\n",
    "            # ]\n",
    "\n",
    "            yield batch_meta_list\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.steps_per_epoch\n",
    "\n",
    "    def state_dict(self) -> Dict:\n",
    "        state = {'pointers_dict': self.pointers_dict, 'indexes_dict': self.indexes_dict}\n",
    "        return state\n",
    "\n",
    "    def load_state_dict(self, state) -> NoReturn:\n",
    "        self.pointers_dict = state['pointers_dict']\n",
    "        self.indexes_dict = state['indexes_dict']\n",
    "\n",
    "\n",
    "class DistributedSamplerWrapper:\n",
    "    def __init__(self, sampler):\n",
    "        r\"\"\"Distributed wrapper of sampler.\"\"\"\n",
    "        self.sampler = sampler\n",
    "\n",
    "    def __iter__(self) -> List[Dict]:\n",
    "\n",
    "        num_replicas = dist.get_world_size()  # number of GPUs.\n",
    "        rank = dist.get_rank()  # rank of current GPU\n",
    "\n",
    "        for batch_meta_list in self.sampler:\n",
    "\n",
    "            # When mix-audio is 2, batch_meta_list looks like: [\n",
    "            #     {'vocals': [\n",
    "            #         {'hdf5_path': 'songA.h5', 'key_in_hdf5': 'vocals', 'begin_sample': 13406400, 'end_sample': 13538700},\n",
    "            #         {'hdf5_path': 'songB.h5', 'key_in_hdf5': 'vocals', 'begin_sample': 4440870, 'end_sample': 4573170}]\n",
    "            #      'accompaniment': [\n",
    "            #         {'hdf5_path': 'songE.h5', 'key_in_hdf5': 'vocals', 'begin_sample': 14579460, 'end_sample': 14711760},\n",
    "            #         {'hdf5_path': 'songF.h5', 'key_in_hdf5': 'vocals', 'begin_sample': 3995460, 'end_sample': 4127760}]\n",
    "            #     }\n",
    "            #     ... (batch_size)\n",
    "            # ]\n",
    "\n",
    "            # Yield a subset of batch_meta_list on one GPU.\n",
    "            yield batch_meta_list[rank::num_replicas]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sampler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA MODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, NoReturn, Optional\n",
    "\n",
    "import h5py\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_lightning.core.datamodule import LightningDataModule\n",
    "\n",
    "\n",
    "class DataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_sampler: object,\n",
    "        train_dataset: object,\n",
    "        num_workers: int,\n",
    "        distributed: bool,\n",
    "    ):\n",
    "        r\"\"\"Data module.\n",
    "\n",
    "        Args:\n",
    "            train_sampler: Sampler object\n",
    "            train_dataset: Dataset object\n",
    "            num_workers: int\n",
    "            distributed: bool\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._train_sampler = train_sampler\n",
    "        self.train_dataset = train_dataset\n",
    "        self.num_workers = num_workers\n",
    "        self.distributed = distributed\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> NoReturn:\n",
    "        r\"\"\"called on every device.\"\"\"\n",
    "\n",
    "        # SegmentSampler is used for sampling segment indexes for training.\n",
    "        # On multiple devices, each SegmentSampler samples a part of mini-batch\n",
    "        # data.\n",
    "\n",
    "        if self.distributed:\n",
    "            self.train_sampler = DistributedSamplerWrapper(self._train_sampler)\n",
    "\n",
    "        else:\n",
    "            self.train_sampler = self._train_sampler\n",
    "\n",
    "    def train_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        r\"\"\"Get train loader.\"\"\"\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset=self.train_dataset,\n",
    "            batch_sampler=self.train_sampler,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        return train_loader\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_source_types: List[str],\n",
    "        target_source_types: List[str],\n",
    "        paired_input_target_data: bool,\n",
    "        input_channels: int,\n",
    "        augmentor: Augmentor,\n",
    "        segment_samples: int,\n",
    "    ):\n",
    "        r\"\"\"Used for getting data according to a meta.\n",
    "\n",
    "        Args:\n",
    "            input_source_types: list of str, e.g., ['vocals', 'accompaniment']\n",
    "            target_source_types: list of str, e.g., ['vocals']\n",
    "            input_channels: int\n",
    "            augmentor: Augmentor\n",
    "            segment_samples: int\n",
    "        \"\"\"\n",
    "        self.input_source_types = input_source_types\n",
    "        self.paired_input_target_data = paired_input_target_data\n",
    "        self.input_channels = input_channels\n",
    "        self.augmentor = augmentor\n",
    "        self.segment_samples = segment_samples\n",
    "\n",
    "        if paired_input_target_data:\n",
    "            self.source_types = list(set(input_source_types) | set(target_source_types))\n",
    "\n",
    "        else:\n",
    "            self.source_types = input_source_types\n",
    "\n",
    "    def __getitem__(self, meta: Dict) -> Dict:\n",
    "        r\"\"\"Return data according to a meta. E.g., an input meta looks like: {\n",
    "            'vocals': [['song_A.h5', 6332760, 6465060], ['song_B.h5', 198450, 330750]],\n",
    "            'accompaniment': [['song_C.h5', 24232920, 24365250], ['song_D.h5', 1569960, 1702260]]}.\n",
    "        }\n",
    "\n",
    "        Then, vocals segments of song_A and song_B will be mixed (mix-audio augmentation).\n",
    "        Accompaniment segments of song_C and song_B will be mixed (mix-audio augmentation).\n",
    "        Finally, mixture is created by summing vocals and accompaniment.\n",
    "\n",
    "        Args:\n",
    "            meta: dict, e.g., {\n",
    "                'vocals': [['song_A.h5', 6332760, 6465060], ['song_B.h5', 198450, 330750]],\n",
    "                'accompaniment': [['song_C.h5', 24232920, 24365250], ['song_D.h5', 1569960, 1702260]]}\n",
    "            }\n",
    "\n",
    "        Returns:\n",
    "            data_dict: dict, e.g., {\n",
    "                'vocals': (channels, segments_num),\n",
    "                'accompaniment': (channels, segments_num),\n",
    "                'mixture': (channels, segments_num),\n",
    "            }\n",
    "        \"\"\"\n",
    "        data_dict = {}\n",
    "\n",
    "        for source_type in self.source_types:\n",
    "            # E.g., ['vocals', 'accompaniment']\n",
    "\n",
    "            waveforms = []  # Audio segments to be mix-audio augmented.\n",
    "\n",
    "            for m in meta[source_type]:\n",
    "                # E.g., {\n",
    "                #     'hdf5_path': '.../song_A.h5',\n",
    "                #     'key_in_hdf5': 'vocals',\n",
    "                #     'begin_sample': '13406400',\n",
    "                #     'end_sample': 13538700,\n",
    "                # }\n",
    "\n",
    "                hdf5_path = m['hdf5_path']\n",
    "                key_in_hdf5 = m['key_in_hdf5']\n",
    "                bgn_sample = m['begin_sample']\n",
    "                end_sample = m['end_sample']\n",
    "\n",
    "                with h5py.File(hdf5_path, 'r') as hf:\n",
    "\n",
    "                    if source_type == 'audioset':\n",
    "                        index_in_hdf5 = m['index_in_hdf5']\n",
    "                        waveform = int16_to_float32(\n",
    "                            hf['waveform'][index_in_hdf5][bgn_sample:end_sample]\n",
    "                        )\n",
    "                        waveform = waveform[None, :]\n",
    "                    else:\n",
    "                        waveform = int16_to_float32(\n",
    "                            hf[key_in_hdf5][:, bgn_sample:end_sample]\n",
    "                        )\n",
    "\n",
    "                if self.paired_input_target_data:\n",
    "                    # TODO\n",
    "                    pass\n",
    "\n",
    "                else:\n",
    "                    if self.augmentor:\n",
    "                        waveform = self.augmentor(waveform, source_type)\n",
    "\n",
    "                if source_type in self.input_source_types:\n",
    "                    waveform = self.match_waveform_to_input_channels(\n",
    "                        waveform=waveform, input_channels=self.input_channels\n",
    "                    )\n",
    "                    # (input_channels, segments_num)\n",
    "\n",
    "                waveform = librosa.util.fix_length(\n",
    "                    waveform, size=self.segment_samples, axis=1\n",
    "                )\n",
    "\n",
    "                waveforms.append(waveform)\n",
    "            # E.g., waveforms: [(input_channels, audio_samples), (input_channels, audio_samples)]\n",
    "\n",
    "            # mix-audio augmentation\n",
    "            data_dict[source_type] = np.sum(waveforms, axis=0)\n",
    "            # data_dict[source_type]: (input_channels, audio_samples)\n",
    "\n",
    "        # data_dict looks like: {\n",
    "        #     'voclas': (input_channels, audio_samples),\n",
    "        #     'accompaniment': (input_channels, audio_samples)\n",
    "        # }\n",
    "\n",
    "        return data_dict\n",
    "\n",
    "    def match_waveform_to_input_channels(\n",
    "        self,\n",
    "        waveform: np.array,\n",
    "        input_channels: int,\n",
    "    ) -> np.array:\n",
    "        r\"\"\"Match waveform to channels num.\n",
    "\n",
    "        Args:\n",
    "            waveform: (input_channels, segments_num)\n",
    "            input_channels: int\n",
    "\n",
    "        Outputs:\n",
    "            output: (new_input_channels, segments_num)\n",
    "        \"\"\"\n",
    "        waveform_channels = waveform.shape[0]\n",
    "\n",
    "        if waveform_channels == input_channels:\n",
    "            return waveform\n",
    "\n",
    "        elif waveform_channels < input_channels:\n",
    "            assert waveform_channels == 1\n",
    "            return np.tile(waveform, (input_channels, 1))\n",
    "\n",
    "        else:\n",
    "            assert input_channels == 1\n",
    "            return np.mean(waveform, axis=0)[None, :]\n",
    "\n",
    "\n",
    "def collate_fn(list_data_dict: List[Dict]) -> Dict:\n",
    "    r\"\"\"Collate mini-batch data to inputs and targets for training.\n",
    "\n",
    "    Args:\n",
    "        list_data_dict: e.g., [\n",
    "            {'vocals': (input_channels, segment_samples),\n",
    "             'accompaniment': (input_channels, segment_samples),\n",
    "             'mixture': (input_channels, segment_samples)\n",
    "            },\n",
    "            {'vocals': (input_channels, segment_samples),\n",
    "             'accompaniment': (input_channels, segment_samples),\n",
    "             'mixture': (input_channels, segment_samples)\n",
    "            },\n",
    "            ...]\n",
    "\n",
    "    Returns:\n",
    "        data_dict: e.g. {\n",
    "            'vocals': (batch_size, input_channels, segment_samples),\n",
    "            'accompaniment': (batch_size, input_channels, segment_samples),\n",
    "            'mixture': (batch_size, input_channels, segment_samples)\n",
    "            }\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "\n",
    "    for key in list_data_dict[0].keys():\n",
    "        data_dict[key] = torch.Tensor(\n",
    "            np.array([data_dict[key] for data_dict in list_data_dict])\n",
    "        )\n",
    "\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pitch_shifted_segment_samples(segment_samples: int, augmentations: Dict) -> int:\n",
    "    r\"\"\"Get new segment samples depending on maximum pitch shift.\n",
    "\n",
    "    Args:\n",
    "        segment_samples: int\n",
    "        augmentations: Dict\n",
    "\n",
    "    Returns:\n",
    "        ex_segment_samples: int\n",
    "    \"\"\"\n",
    "\n",
    "    if 'pitch_shift' not in augmentations.keys():\n",
    "        return segment_samples\n",
    "\n",
    "    else:\n",
    "        pitch_shift_dict = augmentations['pitch_shift']\n",
    "        source_types = pitch_shift_dict.keys()\n",
    "\n",
    "    max_pitch_shift = max(\n",
    "        [pitch_shift_dict[source_type] for source_type in source_types]\n",
    "    )\n",
    "\n",
    "    ex_segment_samples = int(segment_samples * get_pitch_shift_factor(max_pitch_shift))\n",
    "\n",
    "    return ex_segment_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_module(\n",
    "    workspace: str,\n",
    "    config_yaml: str,\n",
    "    num_workers: int,\n",
    "    distributed: bool,\n",
    ") -> DataModule:\n",
    "    r\"\"\"Create data_module. Here is an example to fetch a mini-batch:\n",
    "\n",
    "    code-block:: python\n",
    "\n",
    "        data_module.setup()\n",
    "        for batch_data_dict in data_module.train_dataloader():\n",
    "            print(batch_data_dict.keys())\n",
    "            break\n",
    "\n",
    "    Args:\n",
    "        workspace: str\n",
    "        config_yaml: str\n",
    "        num_workers: int, e.g., 0 for non-parallel and 8 for using cpu cores\n",
    "            for preparing data in parallel\n",
    "        distributed: bool\n",
    "\n",
    "    Returns:\n",
    "        data_module: DataModule\n",
    "    \"\"\"\n",
    "    configs = read_yaml(config_yaml)\n",
    "    input_source_types = configs['train']['input_source_types']\n",
    "    target_source_types = configs['train']['target_source_types']\n",
    "    paired_input_target_data = configs['train']['paired_input_target_data']\n",
    "    indexes_dict_path = os.path.join(workspace, configs['train']['indexes_dict_path'])\n",
    "    sample_rate = configs['train']['sample_rate']\n",
    "    input_channels = configs['train']['input_channels']\n",
    "    segment_seconds = configs['train']['segment_seconds']\n",
    "    augmentations = configs['train']['augmentations']\n",
    "    batch_size = configs['train']['batch_size']\n",
    "    steps_per_epoch = configs['train']['steps_per_epoch']\n",
    "\n",
    "    segment_samples = int(segment_seconds * sample_rate)\n",
    "\n",
    "    if paired_input_target_data:\n",
    "        assert (\n",
    "            augmentations['remixing_sources'] is False\n",
    "        ), \"Must set remixing_sources to False if input and target data are paired.\"\n",
    "\n",
    "    ex_segment_samples = get_pitch_shifted_segment_samples(\n",
    "        segment_samples=segment_samples,\n",
    "        augmentations=augmentations,\n",
    "    )\n",
    "\n",
    "    # sampler\n",
    "    train_sampler = SegmentSampler(\n",
    "        indexes_dict_path=indexes_dict_path,\n",
    "        input_source_types=input_source_types,\n",
    "        target_source_types=target_source_types,\n",
    "        segment_samples=ex_segment_samples,\n",
    "        remixing_sources=augmentations['remixing_sources'],\n",
    "        mixaudio_dict=augmentations['mixaudio'],\n",
    "        batch_size=batch_size,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "    )\n",
    "\n",
    "    # augmentor\n",
    "    augmentor = Augmentor(augmentations=augmentations)\n",
    "\n",
    "    # dataset\n",
    "    train_dataset = Dataset(\n",
    "        input_source_types=input_source_types,\n",
    "        target_source_types=target_source_types,\n",
    "        paired_input_target_data=paired_input_target_data,\n",
    "        input_channels=input_channels,\n",
    "        augmentor=augmentor,\n",
    "        segment_samples=segment_samples,\n",
    "    )\n",
    "\n",
    "    # data module\n",
    "    data_module = DataModule(\n",
    "        train_sampler=train_sampler,\n",
    "        train_dataset=train_dataset,\n",
    "        num_workers=num_workers,\n",
    "        distributed=False,\n",
    "    )\n",
    "\n",
    "    return data_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Data Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MixtureTargetBatchDataPreprocessor(nn.Module):\n",
    "    def __init__(self, input_source_types: List[str], target_source_types: List[str]):\n",
    "        r\"\"\"Batch data preprocessor. Used for preparing mixtures and targets for\n",
    "        training. If there are multiple target source types, the waveforms of\n",
    "        those sources will be stacked along the channel dimension.\n",
    "\n",
    "        Args:\n",
    "            input_source_types: List[str], e.g., ['vocals', 'bass', ...]\n",
    "            target_source_types: List[str], e.g., ['vocals', 'bass', ...]\n",
    "        \"\"\"\n",
    "        super(MixtureTargetBatchDataPreprocessor, self).__init__()\n",
    "\n",
    "        self.input_source_types = input_source_types\n",
    "        self.target_source_types = target_source_types\n",
    "\n",
    "    def __call__(self, batch_data_dict: Dict) -> List[Dict]:\n",
    "        r\"\"\"Format waveforms and targets for training.\n",
    "\n",
    "        Args:\n",
    "            batch_data_dict: dict, e.g., {\n",
    "                'mixture': (batch_size, input_channels, segment_samples),\n",
    "                'vocals': (batch_size, input_channels, segment_samples),\n",
    "                'bass': (batch_size, input_channels, segment_samples),\n",
    "                ...,\n",
    "            }\n",
    "\n",
    "        Returns:\n",
    "            input_dict: dict, e.g., {\n",
    "                'waveform': (batch_size, input_channels, segment_samples),\n",
    "            }\n",
    "            output_dict: dict, e.g., {\n",
    "                'waveform': (batch_size, target_sources_num * output_channels, segment_samples)\n",
    "            }\n",
    "        \"\"\"\n",
    "        # Get mixture. Sum waveforms all sources.\n",
    "        stacked_sources = torch.stack(\n",
    "            [batch_data_dict[source_type] for source_type in self.input_source_types],\n",
    "            dim=1,\n",
    "        )\n",
    "        # input_waveforms: (batch_size, input_sources, input_channels, segment_samples)\n",
    "\n",
    "        input_waveforms = torch.sum(stacked_sources, dim=1)\n",
    "        # input_waveforms: (batch_size, input_channels, segment_samples)\n",
    "\n",
    "        # Concatenate waveforms of multiple targets along the channel axis.\n",
    "        target_waveforms = torch.cat(\n",
    "            [batch_data_dict[source_type] for source_type in self.target_source_types],\n",
    "            dim=1,\n",
    "        )\n",
    "        # target_waveform: (batch_size, target_sources_num * output_channels, segment_samples)\n",
    "\n",
    "        input_dict = {'waveform': input_waveforms}\n",
    "        target_dict = {'waveform': target_waveforms}\n",
    "\n",
    "        return input_dict, target_dict\n",
    "\n",
    "\n",
    "class MixtureTargetConditionalBatchDataPreprocessor:\n",
    "    def __init__(self, input_source_types: List[str], target_source_types: List[str]):\n",
    "        r\"\"\"Conditional single input single output (SISO) batch data\n",
    "        preprocessor. Select one target source from several target sources as\n",
    "        training target and prepare the corresponding conditional vector.\n",
    "\n",
    "        Args:\n",
    "            input_source_types: List[str], e.g., ['vocals', 'bass', ...]\n",
    "            target_source_types: List[str], e.g., ['vocals', 'bass', ...]\n",
    "        \"\"\"\n",
    "        self.input_source_types = input_source_types\n",
    "        self.target_source_types = target_source_types\n",
    "\n",
    "        self.target_sources_num = len(self.target_source_types)\n",
    "\n",
    "    def __call__(self, batch_data_dict: Dict) -> List[Dict]:\n",
    "        r\"\"\"Format waveforms and targets for training.\n",
    "\n",
    "        Args:\n",
    "            batch_data_dict: dict, e.g., {\n",
    "                'mixture': (batch_size, input_channels, segment_samples),\n",
    "                'vocals': (batch_size, input_channels, segment_samples),\n",
    "                'bass': (batch_size, input_channels, segment_samples),\n",
    "                ...,\n",
    "            }\n",
    "\n",
    "        Returns:\n",
    "            input_dict: dict, e.g., {\n",
    "                'waveform': (batch_size, input_channels, segment_samples),\n",
    "                'condition': (batch_size, target_sources_num),\n",
    "            }\n",
    "            output_dict: dict, e.g., {\n",
    "                'waveform': (batch_size, output_channels, segment_samples)\n",
    "            }\n",
    "        \"\"\"\n",
    "        first_source_type = list(batch_data_dict.keys())[0]\n",
    "        batch_size = batch_data_dict[first_source_type].shape[0]\n",
    "\n",
    "        assert (\n",
    "            batch_size % self.target_sources_num == 0\n",
    "        ), \"Batch size should be \\\n",
    "            evenly divided by target sources number.\"\n",
    "\n",
    "        # Get mixture. Sum waveforms all sources.\n",
    "        stacked_sources = torch.stack(\n",
    "            [batch_data_dict[source_type] for source_type in self.input_source_types],\n",
    "            dim=1,\n",
    "        )\n",
    "        # input_waveforms: (batch_size, input_sources, input_channels, segment_samples)\n",
    "\n",
    "        input_waveforms = torch.sum(stacked_sources, dim=1)\n",
    "        # input_waveforms: (batch_size, input_channels, segment_samples)\n",
    "\n",
    "        conditions = torch.zeros(batch_size, self.target_sources_num).to(\n",
    "            input_waveforms.device\n",
    "        )\n",
    "        # conditions: (batch_size, target_sources_num)\n",
    "\n",
    "        target_waveforms = []\n",
    "\n",
    "        for n in range(batch_size):\n",
    "\n",
    "            k = n % self.target_sources_num  # source class index\n",
    "            source_type = self.target_source_types[k]\n",
    "\n",
    "            target_waveforms.append(batch_data_dict[source_type][n])\n",
    "\n",
    "            conditions[n, k] = 1\n",
    "\n",
    "        # conditions will looks like:\n",
    "        # [[1, 0, 0, 0],\n",
    "        #  [0, 1, 0, 0],\n",
    "        #  [0, 0, 1, 0],\n",
    "        #  [0, 0, 0, 1],\n",
    "        #  [1, 0, 0, 0],\n",
    "        #  [0, 1, 0, 0],\n",
    "        #  ...,\n",
    "        # ]\n",
    "\n",
    "        target_waveforms = torch.stack(target_waveforms, dim=0)\n",
    "        # targets: (batch_size, output_channels, segment_samples)\n",
    "\n",
    "        input_dict = {\n",
    "            'waveform': input_waveforms,\n",
    "            'condition': conditions,\n",
    "        }\n",
    "\n",
    "        target_dict = {'waveform': target_waveforms}\n",
    "\n",
    "        return input_dict, target_dict\n",
    "\n",
    "\n",
    "class AmbisonicBinauralBatchDataPreprocessor(nn.Module):\n",
    "    def __init__(self, input_source_types: List[str], target_source_types: List[str]):\n",
    "        r\"\"\"Batch data preprocessor. Used for preparing mixtures and targets for\n",
    "        training. If there are multiple target source types, the waveforms of\n",
    "        those sources will be stacked along the channel dimension.\n",
    "\n",
    "        Args:\n",
    "            input_source_types: List[str], e.g., ['ambisonic']\n",
    "            target_source_types: List[str], e.g., ['binaural']\n",
    "        \"\"\"\n",
    "        super(AmbisonicBinauralBatchDataPreprocessor, self).__init__()\n",
    "\n",
    "        self.input_source_types = input_source_types\n",
    "        self.target_source_types = target_source_types\n",
    "\n",
    "    def __call__(self, batch_data_dict: Dict) -> List[Dict]:\n",
    "        r\"\"\"Format waveforms and targets for training.\n",
    "\n",
    "        Args:\n",
    "            batch_data_dict: dict, e.g., {\n",
    "                'ambisonic': (batch_size, input_channels, segment_samples),\n",
    "                'binaural': (batch_size, output_channels, segment_samples),\n",
    "            }\n",
    "\n",
    "        Returns:\n",
    "            input_dict: dict, e.g., {\n",
    "                'waveform': (batch_size, input_channels, segment_samples),\n",
    "            }\n",
    "            output_dict: dict, e.g., {\n",
    "                'waveform': (batch_size, output_channels, segment_samples)\n",
    "            }\n",
    "        \"\"\"\n",
    "        input_dict = {'waveform': batch_data_dict['ambisonic']}\n",
    "        target_dict = {'waveform': batch_data_dict['binaural']}\n",
    "\n",
    "        return input_dict, target_dict\n",
    "\n",
    "\n",
    "def get_batch_data_preprocessor_class(batch_data_preprocessor_type: str) -> nn.Module:\n",
    "    r\"\"\"Get batch data preprocessor class.\n",
    "\n",
    "    Args:\n",
    "        batch_data_preprocessor_type: str\n",
    "\n",
    "    Returns:\n",
    "        nn.Module\n",
    "    \"\"\"\n",
    "    if batch_data_preprocessor_type == 'MixtureTarget':\n",
    "        return MixtureTargetBatchDataPreprocessor\n",
    "\n",
    "    elif batch_data_preprocessor_type == 'MixtureTargetConditional':\n",
    "        return MixtureTargetConditionalBatchDataPreprocessor\n",
    "\n",
    "    elif batch_data_preprocessor_type == 'AmbisonicBinaural':\n",
    "        return AmbisonicBinauralBatchDataPreprocessor\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, NoReturn\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def init_embedding(layer: nn.Module) -> NoReturn:\n",
    "    r\"\"\"Initialize a Linear or Convolutional layer.\"\"\"\n",
    "    nn.init.uniform_(layer.weight, -1.0, 1.0)\n",
    "\n",
    "    if hasattr(layer, 'bias'):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.0)\n",
    "\n",
    "\n",
    "def init_layer(layer: nn.Module) -> NoReturn:\n",
    "    r\"\"\"Initialize a Linear or Convolutional layer.\"\"\"\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.0)\n",
    "\n",
    "\n",
    "def init_bn(bn: nn.Module) -> NoReturn:\n",
    "    r\"\"\"Initialize a Batchnorm layer.\"\"\"\n",
    "    bn.bias.data.fill_(0.0)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "    bn.running_mean.data.fill_(0.0)\n",
    "    bn.running_var.data.fill_(1.0)\n",
    "\n",
    "\n",
    "def act(x: torch.Tensor, activation: str) -> torch.Tensor:\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        return F.relu_(x)\n",
    "\n",
    "    elif activation == \"leaky_relu\":\n",
    "        return F.leaky_relu_(x, negative_slope=0.01)\n",
    "\n",
    "    elif activation == \"swish\":\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Incorrect activation!\")\n",
    "\n",
    "\n",
    "class Base:\n",
    "    def __init__(self):\n",
    "        r\"\"\"Base function for extracting spectrogram, cos, and sin, etc.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def spectrogram(self, input: torch.Tensor, eps: float = 0.0) -> torch.Tensor:\n",
    "        r\"\"\"Calculate spectrogram.\n",
    "\n",
    "        Args:\n",
    "            input: (batch_size, segments_num)\n",
    "            eps: float\n",
    "\n",
    "        Returns:\n",
    "            spectrogram: (batch_size, time_steps, freq_bins)\n",
    "        \"\"\"\n",
    "        (real, imag) = self.stft(input)\n",
    "        return torch.clamp(real ** 2 + imag ** 2, eps, np.inf) ** 0.5\n",
    "\n",
    "    def spectrogram_phase(\n",
    "        self, input: torch.Tensor, eps: float = 0.0\n",
    "    ) -> List[torch.Tensor]:\n",
    "        r\"\"\"Calculate the magnitude, cos, and sin of the STFT of input.\n",
    "\n",
    "        Args:\n",
    "            input: (batch_size, segments_num)\n",
    "            eps: float\n",
    "\n",
    "        Returns:\n",
    "            mag: (batch_size, time_steps, freq_bins)\n",
    "            cos: (batch_size, time_steps, freq_bins)\n",
    "            sin: (batch_size, time_steps, freq_bins)\n",
    "        \"\"\"\n",
    "        (real, imag) = self.stft(input)\n",
    "        mag = torch.clamp(real ** 2 + imag ** 2, eps, np.inf) ** 0.5\n",
    "        cos = real / mag\n",
    "        sin = imag / mag\n",
    "        return mag, cos, sin\n",
    "\n",
    "    def wav_to_spectrogram_phase(\n",
    "        self, input: torch.Tensor, eps: float = 1e-10\n",
    "    ) -> List[torch.Tensor]:\n",
    "        r\"\"\"Convert waveforms to magnitude, cos, and sin of STFT.\n",
    "\n",
    "        Args:\n",
    "            input: (batch_size, channels_num, segment_samples)\n",
    "            eps: float\n",
    "\n",
    "        Outputs:\n",
    "            mag: (batch_size, channels_num, time_steps, freq_bins)\n",
    "            cos: (batch_size, channels_num, time_steps, freq_bins)\n",
    "            sin: (batch_size, channels_num, time_steps, freq_bins)\n",
    "        \"\"\"\n",
    "        batch_size, channels_num, segment_samples = input.shape\n",
    "\n",
    "        # Reshape input with shapes of (n, segments_num) to meet the\n",
    "        # requirements of the stft function.\n",
    "        x = input.reshape(batch_size * channels_num, segment_samples)\n",
    "\n",
    "        mag, cos, sin = self.spectrogram_phase(x, eps=eps)\n",
    "        # mag, cos, sin: (batch_size * channels_num, 1, time_steps, freq_bins)\n",
    "\n",
    "        _, _, time_steps, freq_bins = mag.shape\n",
    "        mag = mag.reshape(batch_size, channels_num, time_steps, freq_bins)\n",
    "        cos = cos.reshape(batch_size, channels_num, time_steps, freq_bins)\n",
    "        sin = sin.reshape(batch_size, channels_num, time_steps, freq_bins)\n",
    "\n",
    "        return mag, cos, sin\n",
    "\n",
    "    def wav_to_spectrogram(\n",
    "        self, input: torch.Tensor, eps: float = 1e-10\n",
    "    ) -> List[torch.Tensor]:\n",
    "\n",
    "        mag, cos, sin = self.wav_to_spectrogram_phase(input, eps)\n",
    "        return mag\n",
    "\n",
    "\n",
    "class Subband:\n",
    "    def __init__(self, subbands_num: int):\n",
    "        r\"\"\"Warning!! This class is not used!!\n",
    "\n",
    "        This class does not work as good as [1] which split subbands in the\n",
    "        time-domain. Please refer to [1] for formal implementation.\n",
    "\n",
    "        [1] Liu, Haohe, et al. \"Channel-wise subband input for better voice and\n",
    "        accompaniment separation on high resolution music.\" arXiv preprint arXiv:2008.05216 (2020).\n",
    "\n",
    "        Args:\n",
    "            subbands_num: int, e.g., 4\n",
    "        \"\"\"\n",
    "        self.subbands_num = subbands_num\n",
    "\n",
    "    def analysis(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Analysis time-frequency representation into subbands. Stack the\n",
    "        subbands along the channel axis.\n",
    "\n",
    "        Args:\n",
    "            x: (batch_size, channels_num, time_steps, freq_bins)\n",
    "\n",
    "        Returns:\n",
    "            output: (batch_size, channels_num * subbands_num, time_steps, freq_bins // subbands_num)\n",
    "        \"\"\"\n",
    "        batch_size, channels_num, time_steps, freq_bins = x.shape\n",
    "\n",
    "        x = x.reshape(\n",
    "            batch_size,\n",
    "            channels_num,\n",
    "            time_steps,\n",
    "            self.subbands_num,\n",
    "            freq_bins // self.subbands_num,\n",
    "        )\n",
    "        # x: (batch_size, channels_num, time_steps, subbands_num, freq_bins // subbands_num)\n",
    "\n",
    "        x = x.transpose(2, 3)\n",
    "\n",
    "        output = x.reshape(\n",
    "            batch_size,\n",
    "            channels_num * self.subbands_num,\n",
    "            time_steps,\n",
    "            freq_bins // self.subbands_num,\n",
    "        )\n",
    "        # output: (batch_size, channels_num * subbands_num, time_steps, freq_bins // subbands_num)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def synthesis(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Synthesis subband time-frequency representations into original\n",
    "        time-frequency representation.\n",
    "\n",
    "        Args:\n",
    "            x: (batch_size, channels_num * subbands_num, time_steps, freq_bins // subbands_num)\n",
    "\n",
    "        Returns:\n",
    "            output: (batch_size, channels_num, time_steps, freq_bins)\n",
    "        \"\"\"\n",
    "        batch_size, subband_channels_num, time_steps, subband_freq_bins = x.shape\n",
    "\n",
    "        channels_num = subband_channels_num // self.subbands_num\n",
    "        freq_bins = subband_freq_bins * self.subbands_num\n",
    "\n",
    "        x = x.reshape(\n",
    "            batch_size,\n",
    "            channels_num,\n",
    "            self.subbands_num,\n",
    "            time_steps,\n",
    "            subband_freq_bins,\n",
    "        )\n",
    "        # x: (batch_size, channels_num, subbands_num, time_steps, freq_bins // subbands_num)\n",
    "\n",
    "        x = x.transpose(2, 3)\n",
    "        # x: (batch_size, channels_num, time_steps, subbands_num, freq_bins // subbands_num)\n",
    "\n",
    "        output = x.reshape(batch_size, channels_num, time_steps, freq_bins)\n",
    "        # x: (batch_size, channels_num, time_steps, freq_bins)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from inplace_abn.abn import InPlaceABNSync\n",
    "from torchlibrosa.stft import ISTFT, STFT, magphase\n",
    "\n",
    "# from bytesep.models.pytorch_modules import Base, init_bn, init_layer\n",
    "\n",
    "\n",
    "class ConvBlockRes(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, activation, momentum):\n",
    "        r\"\"\"Residual block.\"\"\"\n",
    "        super(ConvBlockRes, self).__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        padding = [kernel_size[0] // 2, kernel_size[1] // 2]\n",
    "\n",
    "        # ABN is not used for bn1 because we found using abn1 will degrade performance.\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels, momentum=momentum)\n",
    "\n",
    "        self.abn2 = InPlaceABNSync(\n",
    "            num_features=out_channels, momentum=momentum, activation='leaky_relu'\n",
    "        )\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=(1, 1),\n",
    "            dilation=(1, 1),\n",
    "            padding=padding,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=(1, 1),\n",
    "            dilation=(1, 1),\n",
    "            padding=padding,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=(1, 1),\n",
    "                stride=(1, 1),\n",
    "                padding=(0, 0),\n",
    "            )\n",
    "            self.is_shortcut = True\n",
    "        else:\n",
    "            self.is_shortcut = False\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_bn(self.bn1)\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "\n",
    "        if self.is_shortcut:\n",
    "            init_layer(self.shortcut)\n",
    "\n",
    "    def forward(self, x):\n",
    "        origin = x\n",
    "        x = self.conv1(F.leaky_relu_(self.bn1(x), negative_slope=0.01))\n",
    "        x = self.conv2(self.abn2(x))\n",
    "\n",
    "        if self.is_shortcut:\n",
    "            return self.shortcut(origin) + x\n",
    "        else:\n",
    "            return origin + x\n",
    "\n",
    "\n",
    "class EncoderBlockRes4B(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size, downsample, activation, momentum\n",
    "    ):\n",
    "        r\"\"\"Encoder block, contains 8 convolutional layers.\"\"\"\n",
    "        super(EncoderBlockRes4B, self).__init__()\n",
    "\n",
    "        self.conv_block1 = ConvBlockRes(\n",
    "            in_channels, out_channels, kernel_size, activation, momentum\n",
    "        )\n",
    "        self.conv_block2 = ConvBlockRes(\n",
    "            out_channels, out_channels, kernel_size, activation, momentum\n",
    "        )\n",
    "        self.conv_block3 = ConvBlockRes(\n",
    "            out_channels, out_channels, kernel_size, activation, momentum\n",
    "        )\n",
    "        self.conv_block4 = ConvBlockRes(\n",
    "            out_channels, out_channels, kernel_size, activation, momentum\n",
    "        )\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder = self.conv_block1(x)\n",
    "        encoder = self.conv_block2(encoder)\n",
    "        encoder = self.conv_block3(encoder)\n",
    "        encoder = self.conv_block4(encoder)\n",
    "        encoder_pool = F.avg_pool2d(encoder, kernel_size=self.downsample)\n",
    "        return encoder_pool, encoder\n",
    "\n",
    "\n",
    "class DecoderBlockRes4B(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size, upsample, activation, momentum\n",
    "    ):\n",
    "        r\"\"\"Decoder block, contains 1 transpose convolutional and 8 convolutional layers.\"\"\"\n",
    "        super(DecoderBlockRes4B, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = upsample\n",
    "        self.activation = activation\n",
    "\n",
    "        self.conv1 = torch.nn.ConvTranspose2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=self.stride,\n",
    "            stride=self.stride,\n",
    "            padding=(0, 0),\n",
    "            bias=False,\n",
    "            dilation=(1, 1),\n",
    "        )\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels, momentum=momentum)\n",
    "        self.conv_block2 = ConvBlockRes(\n",
    "            out_channels * 2, out_channels, kernel_size, activation, momentum\n",
    "        )\n",
    "        self.conv_block3 = ConvBlockRes(\n",
    "            out_channels, out_channels, kernel_size, activation, momentum\n",
    "        )\n",
    "        self.conv_block4 = ConvBlockRes(\n",
    "            out_channels, out_channels, kernel_size, activation, momentum\n",
    "        )\n",
    "        self.conv_block5 = ConvBlockRes(\n",
    "            out_channels, out_channels, kernel_size, activation, momentum\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_bn(self.bn1)\n",
    "        init_layer(self.conv1)\n",
    "\n",
    "    def forward(self, input_tensor, concat_tensor):\n",
    "        x = self.conv1(F.relu_(self.bn1(input_tensor)))\n",
    "        x = torch.cat((x, concat_tensor), dim=1)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.conv_block4(x)\n",
    "        x = self.conv_block5(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResUNet143_DecouplePlusInplaceABN_ISMIR2021(nn.Module, Base):\n",
    "    def __init__(self, input_channels, target_sources_num):\n",
    "        super(ResUNet143_DecouplePlusInplaceABN_ISMIR2021, self).__init__()\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.target_sources_num = target_sources_num\n",
    "\n",
    "        window_size = 2048\n",
    "        hop_size = 441\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        window = 'hann'\n",
    "        activation = 'leaky_relu'\n",
    "        momentum = 0.01\n",
    "\n",
    "        self.subbands_num = 1\n",
    "\n",
    "        assert (\n",
    "            self.subbands_num == 1\n",
    "        ), \"Using subbands_num > 1 on spectrogram \\\n",
    "            will lead to unexpected performance sometimes. Suggest to use \\\n",
    "            subband method on waveform.\"\n",
    "\n",
    "        # Downsample rate along the time axis.\n",
    "        self.K = 4  # outputs: |M|, cos∠M, sin∠M, Q\n",
    "        self.time_downsample_ratio = 2 ** 5  # This number equals 2^{#encoder_blcoks}\n",
    "\n",
    "        self.stft = STFT(\n",
    "            n_fft=window_size,\n",
    "            hop_length=hop_size,\n",
    "            win_length=window_size,\n",
    "            window=window,\n",
    "            center=center,\n",
    "            pad_mode=pad_mode,\n",
    "            freeze_parameters=True,\n",
    "        )\n",
    "\n",
    "        self.istft = ISTFT(\n",
    "            n_fft=window_size,\n",
    "            hop_length=hop_size,\n",
    "            win_length=window_size,\n",
    "            window=window,\n",
    "            center=center,\n",
    "            pad_mode=pad_mode,\n",
    "            freeze_parameters=True,\n",
    "        )\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(window_size // 2 + 1, momentum=momentum)\n",
    "\n",
    "        self.encoder_block1 = EncoderBlockRes4B(\n",
    "            in_channels=input_channels * self.subbands_num,\n",
    "            out_channels=32,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(2, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.encoder_block2 = EncoderBlockRes4B(\n",
    "            in_channels=32,\n",
    "            out_channels=64,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(2, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.encoder_block3 = EncoderBlockRes4B(\n",
    "            in_channels=64,\n",
    "            out_channels=128,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(2, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.encoder_block4 = EncoderBlockRes4B(\n",
    "            in_channels=128,\n",
    "            out_channels=256,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(2, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.encoder_block5 = EncoderBlockRes4B(\n",
    "            in_channels=256,\n",
    "            out_channels=384,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(2, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.encoder_block6 = EncoderBlockRes4B(\n",
    "            in_channels=384,\n",
    "            out_channels=384,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(1, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.conv_block7a = EncoderBlockRes4B(\n",
    "            in_channels=384,\n",
    "            out_channels=384,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(1, 1),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.conv_block7b = EncoderBlockRes4B(\n",
    "            in_channels=384,\n",
    "            out_channels=384,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(1, 1),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.conv_block7c = EncoderBlockRes4B(\n",
    "            in_channels=384,\n",
    "            out_channels=384,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(1, 1),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.conv_block7d = EncoderBlockRes4B(\n",
    "            in_channels=384,\n",
    "            out_channels=384,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(1, 1),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.decoder_block1 = DecoderBlockRes4B(\n",
    "            in_channels=384,\n",
    "            out_channels=384,\n",
    "            kernel_size=(3, 3),\n",
    "            upsample=(1, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.decoder_block2 = DecoderBlockRes4B(\n",
    "            in_channels=384,\n",
    "            out_channels=384,\n",
    "            kernel_size=(3, 3),\n",
    "            upsample=(2, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.decoder_block3 = DecoderBlockRes4B(\n",
    "            in_channels=384,\n",
    "            out_channels=256,\n",
    "            kernel_size=(3, 3),\n",
    "            upsample=(2, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.decoder_block4 = DecoderBlockRes4B(\n",
    "            in_channels=256,\n",
    "            out_channels=128,\n",
    "            kernel_size=(3, 3),\n",
    "            upsample=(2, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.decoder_block5 = DecoderBlockRes4B(\n",
    "            in_channels=128,\n",
    "            out_channels=64,\n",
    "            kernel_size=(3, 3),\n",
    "            upsample=(2, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.decoder_block6 = DecoderBlockRes4B(\n",
    "            in_channels=64,\n",
    "            out_channels=32,\n",
    "            kernel_size=(3, 3),\n",
    "            upsample=(2, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "\n",
    "        self.after_conv_block1 = EncoderBlockRes4B(\n",
    "            in_channels=32,\n",
    "            out_channels=32,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(1, 1),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "\n",
    "        self.after_conv2 = nn.Conv2d(\n",
    "            in_channels=32,\n",
    "            out_channels=target_sources_num\n",
    "            * input_channels\n",
    "            * self.K\n",
    "            * self.subbands_num,\n",
    "            kernel_size=(1, 1),\n",
    "            stride=(1, 1),\n",
    "            padding=(0, 0),\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.after_conv2)\n",
    "\n",
    "    def feature_maps_to_wav(\n",
    "        self,\n",
    "        input_tensor: torch.Tensor,\n",
    "        sp: torch.Tensor,\n",
    "        sin_in: torch.Tensor,\n",
    "        cos_in: torch.Tensor,\n",
    "        audio_length: int,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Convert feature maps to waveform.\n",
    "\n",
    "        Args:\n",
    "            input_tensor: (batch_size, target_sources_num * input_channels * self.K, time_steps, freq_bins)\n",
    "            sp: (batch_size, target_sources_num * input_channels, time_steps, freq_bins)\n",
    "            sin_in: (batch_size, target_sources_num * input_channels, time_steps, freq_bins)\n",
    "            cos_in: (batch_size, target_sources_num * input_channels, time_steps, freq_bins)\n",
    "\n",
    "        Outputs:\n",
    "            waveform: (batch_size, target_sources_num * input_channels, segment_samples)\n",
    "        \"\"\"\n",
    "        batch_size, _, time_steps, freq_bins = input_tensor.shape\n",
    "\n",
    "        x = input_tensor.reshape(\n",
    "            batch_size,\n",
    "            self.target_sources_num,\n",
    "            self.input_channels,\n",
    "            self.K,\n",
    "            time_steps,\n",
    "            freq_bins,\n",
    "        )\n",
    "        # x: (batch_size, target_sources_num, input_channles, K, time_steps, freq_bins)\n",
    "\n",
    "        mask_mag = torch.sigmoid(x[:, :, :, 0, :, :])\n",
    "        _mask_real = torch.tanh(x[:, :, :, 1, :, :])\n",
    "        _mask_imag = torch.tanh(x[:, :, :, 2, :, :])\n",
    "        _, mask_cos, mask_sin = magphase(_mask_real, _mask_imag)\n",
    "        linear_mag = x[:, :, :, 3, :, :]\n",
    "        # mask_cos, mask_sin: (batch_size, target_sources_num, input_channles, time_steps, freq_bins)\n",
    "\n",
    "        # Y = |Y|cos∠Y + j|Y|sin∠Y\n",
    "        #   = |Y|cos(∠X + ∠M) + j|Y|sin(∠X + ∠M)\n",
    "        #   = |Y|(cos∠X cos∠M - sin∠X sin∠M) + j|Y|(sin∠X cos∠M + cos∠X sin∠M)\n",
    "        out_cos = (\n",
    "            cos_in[:, None, :, :, :] * mask_cos - sin_in[:, None, :, :, :] * mask_sin\n",
    "        )\n",
    "        out_sin = (\n",
    "            sin_in[:, None, :, :, :] * mask_cos + cos_in[:, None, :, :, :] * mask_sin\n",
    "        )\n",
    "        # out_cos: (batch_size, target_sources_num, input_channles, time_steps, freq_bins)\n",
    "        # out_sin: (batch_size, target_sources_num, input_channles, time_steps, freq_bins)\n",
    "\n",
    "        # Calculate |Y|.\n",
    "        out_mag = F.relu_(sp[:, None, :, :, :] * mask_mag + linear_mag)\n",
    "        # out_mag: (batch_size, target_sources_num, input_channles, time_steps, freq_bins)\n",
    "\n",
    "        # Calculate Y_{real} and Y_{imag} for ISTFT.\n",
    "        out_real = out_mag * out_cos\n",
    "        out_imag = out_mag * out_sin\n",
    "        # out_real, out_imag: (batch_size, target_sources_num, input_channles, time_steps, freq_bins)\n",
    "\n",
    "        # Reformat shape to (n, 1, time_steps, freq_bins) for ISTFT.\n",
    "        shape = (\n",
    "            batch_size * self.target_sources_num * self.input_channels,\n",
    "            1,\n",
    "            time_steps,\n",
    "            freq_bins,\n",
    "        )\n",
    "        out_real = out_real.reshape(shape)\n",
    "        out_imag = out_imag.reshape(shape)\n",
    "\n",
    "        # ISTFT.\n",
    "        x = self.istft(out_real, out_imag, audio_length)\n",
    "        # (batch_size * target_sources_num * input_channels, segments_num)\n",
    "\n",
    "        # Reshape.\n",
    "        waveform = x.reshape(\n",
    "            batch_size, self.target_sources_num * self.input_channels, audio_length\n",
    "        )\n",
    "        # (batch_size, target_sources_num * input_channels, segments_num)\n",
    "\n",
    "        return waveform\n",
    "\n",
    "    def forward(self, input_dict):\n",
    "        r\"\"\"Forward data into the module.\n",
    "\n",
    "        Args:\n",
    "            input_dict: dict, e.g., {\n",
    "                waveform: (batch_size, input_channels, segment_samples),\n",
    "                ...,\n",
    "            }\n",
    "\n",
    "        Outputs:\n",
    "            output_dict: dict, e.g., {\n",
    "                'waveform': (batch_size, input_channels, segment_samples),\n",
    "                ...,\n",
    "            }\n",
    "        \"\"\"\n",
    "        mixtures = input_dict['waveform']\n",
    "        # (batch_size, input_channels, segment_samples)\n",
    "\n",
    "        mag, cos_in, sin_in = self.wav_to_spectrogram_phase(mixtures)\n",
    "        # mag, cos_in, sin_in: (batch_size, input_channels, time_steps, freq_bins)\n",
    "\n",
    "        # Batch normalize on individual frequency bins.\n",
    "        x = mag.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        # x: (batch_size, input_channels, time_steps, freq_bins)\n",
    "\n",
    "        # Pad spectrogram to be evenly divided by downsample ratio.\n",
    "        origin_len = x.shape[2]\n",
    "        pad_len = (\n",
    "            int(np.ceil(x.shape[2] / self.time_downsample_ratio))\n",
    "            * self.time_downsample_ratio\n",
    "            - origin_len\n",
    "        )\n",
    "        x = F.pad(x, pad=(0, 0, 0, pad_len))\n",
    "        # (batch_size, channels, padded_time_steps, freq_bins)\n",
    "\n",
    "        # Let frequency bins be evenly divided by 2, e.g., 1025 -> 1024.\n",
    "        x = x[..., 0 : x.shape[-1] - 1]  # (bs, channels, T, F)\n",
    "\n",
    "        if self.subbands_num > 1:\n",
    "            x = self.subband.analysis(x)\n",
    "            # (bs, input_channels, T, F'), where F' = F // subbands_num\n",
    "\n",
    "        # UNet\n",
    "        (x1_pool, x1) = self.encoder_block1(x)  # x1_pool: (bs, 32, T / 2, F / 2)\n",
    "        (x2_pool, x2) = self.encoder_block2(x1_pool)  # x2_pool: (bs, 64, T / 4, F / 4)\n",
    "        (x3_pool, x3) = self.encoder_block3(x2_pool)  # x3_pool: (bs, 128, T / 8, F / 8)\n",
    "        (x4_pool, x4) = self.encoder_block4(\n",
    "            x3_pool\n",
    "        )  # x4_pool: (bs, 256, T / 16, F / 16)\n",
    "        (x5_pool, x5) = self.encoder_block5(\n",
    "            x4_pool\n",
    "        )  # x5_pool: (bs, 384, T / 32, F / 32)\n",
    "        (x6_pool, x6) = self.encoder_block6(\n",
    "            x5_pool\n",
    "        )  # x6_pool: (bs, 384, T / 32, F / 64)\n",
    "        (x_center, _) = self.conv_block7a(x6_pool)  # (bs, 384, T / 32, F / 64)\n",
    "        (x_center, _) = self.conv_block7b(x_center)  # (bs, 384, T / 32, F / 64)\n",
    "        (x_center, _) = self.conv_block7c(x_center)  # (bs, 384, T / 32, F / 64)\n",
    "        (x_center, _) = self.conv_block7d(x_center)  # (bs, 384, T / 32, F / 64)\n",
    "        x7 = self.decoder_block1(x_center, x6)  # (bs, 384, T / 32, F / 32)\n",
    "        x8 = self.decoder_block2(x7, x5)  # (bs, 384, T / 16, F / 16)\n",
    "        x9 = self.decoder_block3(x8, x4)  # (bs, 256, T / 8, F / 8)\n",
    "        x10 = self.decoder_block4(x9, x3)  # (bs, 128, T / 4, F / 4)\n",
    "        x11 = self.decoder_block5(x10, x2)  # (bs, 64, T / 2, F / 2)\n",
    "        x12 = self.decoder_block6(x11, x1)  # (bs, 32, T, F)\n",
    "        (x, _) = self.after_conv_block1(x12)  # (bs, 32, T, F)\n",
    "\n",
    "        x = self.after_conv2(x)  # (bs, channels * 3, T, F)\n",
    "        # (batch_size, target_sources_num * input_channles * self.K * subbands_num, T, F')\n",
    "\n",
    "        if self.subbands_num > 1:\n",
    "            x = self.subband.synthesis(x)\n",
    "            # (batch_size, target_sources_num * input_channles * self.K, T, F)\n",
    "\n",
    "        # Recover shape\n",
    "        x = F.pad(x, pad=(0, 1))  # Pad frequency, e.g., 1024 -> 1025.\n",
    "\n",
    "        x = x[:, :, 0:origin_len, :]\n",
    "        # (batch_size, target_sources_num * input_channles * self.K, T, F)\n",
    "\n",
    "        audio_length = mixtures.shape[2]\n",
    "\n",
    "        separated_audio = self.feature_maps_to_wav(x, mag, sin_in, cos_in, audio_length)\n",
    "        # separated_audio: (batch_size, target_sources_num * input_channels, segments_num)\n",
    "\n",
    "        output_dict = {'waveform': separated_audio}\n",
    "\n",
    "        return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright The PyTorch Lightning team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Architecture based on U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "    Link - https://arxiv.org/abs/1505.04597\n",
    "\n",
    "    >>> UNet(num_classes=2, num_layers=3)  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "    UNet(\n",
    "      (layers): ModuleList(\n",
    "        (0): DoubleConv(...)\n",
    "        (1): Down(...)\n",
    "        (2): Down(...)\n",
    "        (3): Up(...)\n",
    "        (4): Up(...)\n",
    "        (5): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
    "      )\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = 19,\n",
    "        num_layers: int = 5,\n",
    "        features_start: int = 64,\n",
    "        bilinear: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_classes: Number of output classes required (default 19 for KITTI dataset)\n",
    "            num_layers: Number of layers in each side of U-net\n",
    "            features_start: Number of features in first layer\n",
    "            bilinear: Whether to use bilinear interpolation or transposed convolutions for upsampling.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        layers = [DoubleConv(3, features_start)]\n",
    "\n",
    "        feats = features_start\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(Down(feats, feats * 2))\n",
    "            feats *= 2\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(Up(feats, feats // 2, bilinear))\n",
    "            feats //= 2\n",
    "\n",
    "        layers.append(nn.Conv2d(feats, num_classes, kernel_size=1))\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xi = [self.layers[0](x)]\n",
    "        # Down path\n",
    "        for layer in self.layers[1:self.num_layers]:\n",
    "            xi.append(layer(xi[-1]))\n",
    "        # Up path\n",
    "        for i, layer in enumerate(self.layers[self.num_layers:-1]):\n",
    "            xi[-1] = layer(xi[-1], xi[-2 - i])\n",
    "        return self.layers[-1](xi[-1])\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Double Convolution and BN and ReLU\n",
    "    (3x3 conv -> BN -> ReLU) ** 2\n",
    "\n",
    "    >>> DoubleConv(4, 4)  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "    DoubleConv(\n",
    "      (net): Sequential(...)\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"\n",
    "    Combination of MaxPool2d and DoubleConv in series\n",
    "\n",
    "    >>> Down(4, 8)  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "    Down(\n",
    "      (net): Sequential(\n",
    "        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        (1): DoubleConv(\n",
    "          (net): Sequential(...)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.MaxPool2d(kernel_size=2, stride=2), DoubleConv(in_ch, out_ch))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsampling (by either bilinear interpolation or transpose convolutions)\n",
    "    followed by concatenation of feature map from contracting path,\n",
    "    followed by double 3x3 convolution.\n",
    "\n",
    "    >>> Up(8, 4)  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "    Up(\n",
    "      (upsample): ConvTranspose2d(8, 4, kernel_size=(2, 2), stride=(2, 2))\n",
    "      (conv): DoubleConv(\n",
    "        (net): Sequential(...)\n",
    "      )\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch: int, out_ch: int, bilinear: bool = False):\n",
    "        super().__init__()\n",
    "        self.upsample = None\n",
    "        if bilinear:\n",
    "            self.upsample = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True),\n",
    "                nn.Conv2d(in_ch, in_ch // 2, kernel_size=1),\n",
    "            )\n",
    "        else:\n",
    "            self.upsample = nn.ConvTranspose2d(in_ch, in_ch // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.upsample(x1)\n",
    "\n",
    "        # Pad x1 to the size of x2\n",
    "        diff_h = x2.shape[2] - x1.shape[2]\n",
    "        diff_w = x2.shape[3] - x1.shape[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diff_w // 2, diff_w - diff_w // 2, diff_h // 2, diff_h - diff_h // 2])\n",
    "\n",
    "        # Concatenate along the channels axis\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os.path as op\n",
    "import pathlib\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "\n",
    "\n",
    "def load_mat2numpy(fname=\"\"):\n",
    "    '''\n",
    "    Args:\n",
    "        fname: pth to mat\n",
    "        type:\n",
    "    Returns: dic object\n",
    "    '''\n",
    "    if len(fname) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return loadmat(fname)\n",
    "\n",
    "\n",
    "class PQMF(nn.Module):\n",
    "    def __init__(self, N, M, project_root):\n",
    "        super().__init__()\n",
    "        self.N = N  # nsubband\n",
    "        self.M = M  # nfilter\n",
    "        try:\n",
    "            assert (N, M) in [(8, 64), (4, 64), (2, 64)]\n",
    "        except:\n",
    "            print(\"Warning:\", N, \"subbandand \", M, \" filter is not supported\")\n",
    "        self.pad_samples = 64\n",
    "        self.name = str(N) + \"_\" + str(M) + \".mat\"\n",
    "        self.ana_conv_filter = nn.Conv1d(\n",
    "            1, out_channels=N, kernel_size=M, stride=N, bias=False\n",
    "        )\n",
    "\n",
    "        filters_dir = '{}/bytesep_data/filters'.format(str(pathlib.Path.home()))\n",
    "\n",
    "        for _name in ['f_4_64.mat', 'h_4_64.mat']:\n",
    "\n",
    "            _path = os.path.join(filters_dir, _name)\n",
    "\n",
    "            if not os.path.isfile(_path):\n",
    "                os.makedirs(os.path.dirname(_path), exist_ok=True)\n",
    "                remote_path = (\n",
    "                    \"https://zenodo.org/record/5513378/files/{}?download=1\".format(\n",
    "                        _name\n",
    "                    )\n",
    "                )\n",
    "                command_str = 'wget -O \"{}\" \"{}\"'.format(_path, remote_path)\n",
    "                os.system(command_str)\n",
    "\n",
    "        data = load_mat2numpy(op.join(filters_dir, \"f_\" + self.name))\n",
    "        data = data['f'].astype(np.float32) / N\n",
    "        data = np.flipud(data.T).T\n",
    "        data = np.reshape(data, (N, 1, M)).copy()\n",
    "        dict_new = self.ana_conv_filter.state_dict().copy()\n",
    "        dict_new['weight'] = torch.from_numpy(data)\n",
    "        self.ana_pad = nn.ConstantPad1d((M - N, 0), 0)\n",
    "        self.ana_conv_filter.load_state_dict(dict_new)\n",
    "\n",
    "        self.syn_pad = nn.ConstantPad1d((0, M // N - 1), 0)\n",
    "        self.syn_conv_filter = nn.Conv1d(\n",
    "            N, out_channels=N, kernel_size=M // N, stride=1, bias=False\n",
    "        )\n",
    "        gk = load_mat2numpy(op.join(filters_dir, \"h_\" + self.name))\n",
    "        gk = gk['h'].astype(np.float32)\n",
    "        gk = np.transpose(np.reshape(gk, (N, M // N, N)), (1, 0, 2)) * N\n",
    "        gk = np.transpose(gk[::-1, :, :], (2, 1, 0)).copy()\n",
    "        dict_new = self.syn_conv_filter.state_dict().copy()\n",
    "        dict_new['weight'] = torch.from_numpy(gk)\n",
    "        self.syn_conv_filter.load_state_dict(dict_new)\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def __analysis_channel(self, inputs):\n",
    "        return self.ana_conv_filter(self.ana_pad(inputs))\n",
    "\n",
    "    def __systhesis_channel(self, inputs):\n",
    "        ret = self.syn_conv_filter(self.syn_pad(inputs)).permute(0, 2, 1)\n",
    "        return torch.reshape(ret, (ret.shape[0], 1, -1))\n",
    "\n",
    "    def analysis(self, inputs):\n",
    "        '''\n",
    "        :param inputs: [batchsize,channel,raw_wav],value:[0,1]\n",
    "        :return:\n",
    "        '''\n",
    "        inputs = F.pad(inputs, ((0, self.pad_samples)))\n",
    "        ret = None\n",
    "        for i in range(inputs.size()[1]):  # channels\n",
    "            if ret is None:\n",
    "                ret = self.__analysis_channel(inputs[:, i : i + 1, :])\n",
    "            else:\n",
    "                ret = torch.cat(\n",
    "                    (ret, self.__analysis_channel(inputs[:, i : i + 1, :])), dim=1\n",
    "                )\n",
    "        return ret\n",
    "\n",
    "    def synthesis(self, data):\n",
    "        '''\n",
    "        :param data: [batchsize,self.N*K,raw_wav_sub],value:[0,1]\n",
    "        :return:\n",
    "        '''\n",
    "        ret = None\n",
    "        # data = F.pad(data,((0,self.pad_samples//self.N)))\n",
    "        for i in range(data.size()[1]):  # channels\n",
    "            if i % self.N == 0:\n",
    "                if ret is None:\n",
    "                    ret = self.__systhesis_channel(data[:, i : i + self.N, :])\n",
    "                else:\n",
    "                    new = self.__systhesis_channel(data[:, i : i + self.N, :])\n",
    "                    ret = torch.cat((ret, new), dim=1)\n",
    "        ret = ret[..., : -self.pad_samples]\n",
    "        return ret\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.ana_conv_filter(self.ana_pad(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, NoReturn, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchlibrosa.stft import ISTFT, STFT, magphase\n",
    "\n",
    "# from bytesep.models.pytorch_modules import Base, init_bn, init_layer\n",
    "# from bytesep.models.subband_tools.pqmf import PQMF\n",
    "\n",
    "\n",
    "class ConvBlockRes(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Tuple,\n",
    "        activation: str,\n",
    "        momentum: float,\n",
    "    ):\n",
    "        r\"\"\"Residual block.\"\"\"\n",
    "        super(ConvBlockRes, self).__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        padding = [kernel_size[0] // 2, kernel_size[1] // 2]\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels, momentum=momentum)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels, momentum=momentum)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=(1, 1),\n",
    "            dilation=(1, 1),\n",
    "            padding=padding,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=(1, 1),\n",
    "            dilation=(1, 1),\n",
    "            padding=padding,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=(1, 1),\n",
    "                stride=(1, 1),\n",
    "                padding=(0, 0),\n",
    "            )\n",
    "            self.is_shortcut = True\n",
    "        else:\n",
    "            self.is_shortcut = False\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> NoReturn:\n",
    "        r\"\"\"Initialize weights.\"\"\"\n",
    "        init_bn(self.bn1)\n",
    "        init_bn(self.bn2)\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "\n",
    "        if self.is_shortcut:\n",
    "            init_layer(self.shortcut)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Forward data into the module.\n",
    "\n",
    "        Args:\n",
    "            input_tensor: (batch_size, input_feature_maps, time_steps, freq_bins)\n",
    "\n",
    "        Returns:\n",
    "            output_tensor: (batch_size, output_feature_maps, time_steps, freq_bins)\n",
    "        \"\"\"\n",
    "        x = self.conv1(F.leaky_relu_(self.bn1(input_tensor), negative_slope=0.01))\n",
    "        x = self.conv2(F.leaky_relu_(self.bn2(x), negative_slope=0.01))\n",
    "\n",
    "        if self.is_shortcut:\n",
    "            return self.shortcut(input_tensor) + x\n",
    "        else:\n",
    "            return input_tensor + x\n",
    "\n",
    "\n",
    "class EncoderBlockRes4B(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Tuple,\n",
    "        downsample: Tuple,\n",
    "        activation: str,\n",
    "        momentum: float,\n",
    "    ):\n",
    "        r\"\"\"Encoder block, contains 8 convolutional layers.\"\"\"\n",
    "        super(EncoderBlockRes4B, self).__init__()\n",
    "\n",
    "        self.conv_block1 = ConvBlockRes(\n",
    "            in_channels, out_channels, kernel_size, activation, momentum\n",
    "        )\n",
    "        self.conv_block2 = ConvBlockRes(\n",
    "            out_channels, out_channels, kernel_size, activation, momentum\n",
    "        )\n",
    "        self.conv_block3 = ConvBlockRes(\n",
    "            out_channels, out_channels, kernel_size, activation, momentum\n",
    "        )\n",
    "        self.conv_block4 = ConvBlockRes(\n",
    "            out_channels, out_channels, kernel_size, activation, momentum\n",
    "        )\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Forward data into the module.\n",
    "\n",
    "        Args:\n",
    "            input_tensor: (batch_size, input_feature_maps, time_steps, freq_bins)\n",
    "\n",
    "        Returns:\n",
    "            encoder_pool: (batch_size, output_feature_maps, downsampled_time_steps, downsampled_freq_bins)\n",
    "            encoder: (batch_size, output_feature_maps, time_steps, freq_bins)\n",
    "        \"\"\"\n",
    "        encoder = self.conv_block1(input_tensor)\n",
    "        encoder = self.conv_block2(encoder)\n",
    "        encoder = self.conv_block3(encoder)\n",
    "        encoder = self.conv_block4(encoder)\n",
    "        encoder_pool = F.avg_pool2d(encoder, kernel_size=self.downsample)\n",
    "        return encoder_pool, encoder\n",
    "\n",
    "\n",
    "class DecoderBlockRes4B(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Tuple,\n",
    "        upsample: Tuple,\n",
    "        activation: str,\n",
    "        momentum: float,\n",
    "    ):\n",
    "        r\"\"\"Decoder block, contains 1 transposed convolutional and 8 convolutional layers.\"\"\"\n",
    "        super(DecoderBlockRes4B, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = upsample\n",
    "        self.activation = activation\n",
    "\n",
    "        self.conv1 = torch.nn.ConvTranspose2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=self.stride,\n",
    "            stride=self.stride,\n",
    "            padding=(0, 0),\n",
    "            bias=False,\n",
    "            dilation=(1, 1),\n",
    "        )\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels, momentum=momentum)\n",
    "        self.conv_block2 = ConvBlockRes(\n",
    "            out_channels * 2, out_channels, kernel_size, activation, momentum\n",
    "        )\n",
    "        self.conv_block3 = ConvBlockRes(\n",
    "            out_channels, out_channels, kernel_size, activation, momentum\n",
    "        )\n",
    "        self.conv_block4 = ConvBlockRes(\n",
    "            out_channels, out_channels, kernel_size, activation, momentum\n",
    "        )\n",
    "        self.conv_block5 = ConvBlockRes(\n",
    "            out_channels, out_channels, kernel_size, activation, momentum\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        r\"\"\"Initialize weights.\"\"\"\n",
    "        init_bn(self.bn1)\n",
    "        init_layer(self.conv1)\n",
    "\n",
    "    def forward(\n",
    "        self, input_tensor: torch.Tensor, concat_tensor: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Forward data into the module.\n",
    "\n",
    "        Args:\n",
    "            input_tensor: (batch_size, input_feature_maps, downsampled_time_steps, downsampled_freq_bins)\n",
    "            concat_tensor: (batch_size, input_feature_maps, time_steps, freq_bins)\n",
    "\n",
    "        Returns:\n",
    "            output_tensor: (batch_size, output_feature_maps, time_steps, freq_bins)\n",
    "        \"\"\"\n",
    "        x = self.conv1(F.relu_(self.bn1(input_tensor)))\n",
    "        # (batch_size, input_feature_maps, time_steps, freq_bins)\n",
    "\n",
    "        x = torch.cat((x, concat_tensor), dim=1)\n",
    "        # (batch_size, input_feature_maps * 2, time_steps, freq_bins)\n",
    "\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.conv_block4(x)\n",
    "        x = self.conv_block5(x)\n",
    "        # output_tensor: (batch_size, output_feature_maps, time_steps, freq_bins)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResUNet143_Subbandtime(nn.Module, Base):\n",
    "    def __init__(\n",
    "        self, input_channels: int, output_channels: int, target_sources_num: int\n",
    "    ):\n",
    "        super(ResUNet143_Subbandtime, self).__init__()\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.target_sources_num = target_sources_num\n",
    "\n",
    "        window_size = 512  # 2048 // 4\n",
    "        hop_size = 110  # 441 // 4\n",
    "        center = True\n",
    "        pad_mode = \"reflect\"\n",
    "        window = \"hann\"\n",
    "        activation = \"leaky_relu\"\n",
    "        momentum = 0.01\n",
    "\n",
    "        self.subbands_num = 4\n",
    "        self.K = 4  # outputs: |M|, cos∠M, sin∠M, Q\n",
    "\n",
    "        self.time_downsample_ratio = 2 ** 5  # This number equals 2^{#encoder_blcoks}\n",
    "\n",
    "        self.pqmf = PQMF(\n",
    "            N=self.subbands_num,\n",
    "            M=64,\n",
    "            project_root='bytesep/models/subband_tools/filters',\n",
    "        )\n",
    "\n",
    "        self.stft = STFT(\n",
    "            n_fft=window_size,\n",
    "            hop_length=hop_size,\n",
    "            win_length=window_size,\n",
    "            window=window,\n",
    "            center=center,\n",
    "            pad_mode=pad_mode,\n",
    "            freeze_parameters=True,\n",
    "        )\n",
    "\n",
    "        self.istft = ISTFT(\n",
    "            n_fft=window_size,\n",
    "            hop_length=hop_size,\n",
    "            win_length=window_size,\n",
    "            window=window,\n",
    "            center=center,\n",
    "            pad_mode=pad_mode,\n",
    "            freeze_parameters=True,\n",
    "        )\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(window_size // 2 + 1, momentum=momentum)\n",
    "\n",
    "        self.encoder_block1 = EncoderBlockRes4B(\n",
    "            in_channels=self.input_channels * self.subbands_num,\n",
    "            out_channels=32,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(2, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.encoder_block2 = EncoderBlockRes4B(\n",
    "            in_channels=32,\n",
    "            out_channels=64,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(2, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.encoder_block3 = EncoderBlockRes4B(\n",
    "            in_channels=64,\n",
    "            out_channels=128,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(2, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.encoder_block4 = EncoderBlockRes4B(\n",
    "            in_channels=128,\n",
    "            out_channels=256,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(2, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.encoder_block5 = EncoderBlockRes4B(\n",
    "            in_channels=256,\n",
    "            out_channels=384,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(2, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.encoder_block6 = EncoderBlockRes4B(\n",
    "            in_channels=384,\n",
    "            out_channels=384,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(1, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.conv_block7a = EncoderBlockRes4B(\n",
    "            in_channels=384,\n",
    "            out_channels=384,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(1, 1),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.conv_block7b = EncoderBlockRes4B(\n",
    "            in_channels=384,\n",
    "            out_channels=384,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(1, 1),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.conv_block7c = EncoderBlockRes4B(\n",
    "            in_channels=384,\n",
    "            out_channels=384,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(1, 1),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.conv_block7d = EncoderBlockRes4B(\n",
    "            in_channels=384,\n",
    "            out_channels=384,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(1, 1),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.decoder_block1 = DecoderBlockRes4B(\n",
    "            in_channels=384,\n",
    "            out_channels=384,\n",
    "            kernel_size=(3, 3),\n",
    "            upsample=(1, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.decoder_block2 = DecoderBlockRes4B(\n",
    "            in_channels=384,\n",
    "            out_channels=384,\n",
    "            kernel_size=(3, 3),\n",
    "            upsample=(2, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.decoder_block3 = DecoderBlockRes4B(\n",
    "            in_channels=384,\n",
    "            out_channels=256,\n",
    "            kernel_size=(3, 3),\n",
    "            upsample=(2, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.decoder_block4 = DecoderBlockRes4B(\n",
    "            in_channels=256,\n",
    "            out_channels=128,\n",
    "            kernel_size=(3, 3),\n",
    "            upsample=(2, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.decoder_block5 = DecoderBlockRes4B(\n",
    "            in_channels=128,\n",
    "            out_channels=64,\n",
    "            kernel_size=(3, 3),\n",
    "            upsample=(2, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "        self.decoder_block6 = DecoderBlockRes4B(\n",
    "            in_channels=64,\n",
    "            out_channels=32,\n",
    "            kernel_size=(3, 3),\n",
    "            upsample=(2, 2),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "\n",
    "        self.after_conv_block1 = EncoderBlockRes4B(\n",
    "            in_channels=32,\n",
    "            out_channels=32,\n",
    "            kernel_size=(3, 3),\n",
    "            downsample=(1, 1),\n",
    "            activation=activation,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "\n",
    "        self.after_conv2 = nn.Conv2d(\n",
    "            in_channels=32,\n",
    "            out_channels=self.target_sources_num\n",
    "            * self.output_channels\n",
    "            * self.K\n",
    "            * self.subbands_num,\n",
    "            kernel_size=(1, 1),\n",
    "            stride=(1, 1),\n",
    "            padding=(0, 0),\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        r\"\"\"Initialize weights.\"\"\"\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.after_conv2)\n",
    "\n",
    "    def feature_maps_to_wav(\n",
    "        self,\n",
    "        input_tensor: torch.Tensor,\n",
    "        sp: torch.Tensor,\n",
    "        sin_in: torch.Tensor,\n",
    "        cos_in: torch.Tensor,\n",
    "        audio_length: int,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Convert feature maps to waveform.\n",
    "\n",
    "        Args:\n",
    "            input_tensor: (batch_size, target_sources_num * output_channels * self.K, time_steps, freq_bins)\n",
    "            sp: (batch_size, input_channels, time_steps, freq_bins)\n",
    "            sin_in: (batch_size, input_channels, time_steps, freq_bins)\n",
    "            cos_in: (batch_size, input_channels, time_steps, freq_bins)\n",
    "\n",
    "            (There is input_channels == output_channels for the source separation task.)\n",
    "\n",
    "        Outputs:\n",
    "            waveform: (batch_size, target_sources_num * output_channels, segment_samples)\n",
    "        \"\"\"\n",
    "        batch_size, _, time_steps, freq_bins = input_tensor.shape\n",
    "\n",
    "        x = input_tensor.reshape(\n",
    "            batch_size,\n",
    "            self.target_sources_num,\n",
    "            self.output_channels,\n",
    "            self.K,\n",
    "            time_steps,\n",
    "            freq_bins,\n",
    "        )\n",
    "        # x: (batch_size, target_sources_num, output_channels, self.K, time_steps, freq_bins)\n",
    "\n",
    "        mask_mag = torch.sigmoid(x[:, :, :, 0, :, :])\n",
    "        _mask_real = torch.tanh(x[:, :, :, 1, :, :])\n",
    "        _mask_imag = torch.tanh(x[:, :, :, 2, :, :])\n",
    "        linear_mag = torch.tanh(x[:, :, :, 3, :, :])\n",
    "        _, mask_cos, mask_sin = magphase(_mask_real, _mask_imag)\n",
    "        # mask_cos, mask_sin: (batch_size, target_sources_num, output_channels, time_steps, freq_bins)\n",
    "\n",
    "        # Y = |Y|cos∠Y + j|Y|sin∠Y\n",
    "        #   = |Y|cos(∠X + ∠M) + j|Y|sin(∠X + ∠M)\n",
    "        #   = |Y|(cos∠X cos∠M - sin∠X sin∠M) + j|Y|(sin∠X cos∠M + cos∠X sin∠M)\n",
    "        out_cos = (\n",
    "            cos_in[:, None, :, :, :] * mask_cos - sin_in[:, None, :, :, :] * mask_sin\n",
    "        )\n",
    "        out_sin = (\n",
    "            sin_in[:, None, :, :, :] * mask_cos + cos_in[:, None, :, :, :] * mask_sin\n",
    "        )\n",
    "        # out_cos: (batch_size, target_sources_num, output_channels, time_steps, freq_bins)\n",
    "        # out_sin: (batch_size, target_sources_num, output_channels, time_steps, freq_bins)\n",
    "\n",
    "        # Calculate |Y|.\n",
    "        out_mag = F.relu_(sp[:, None, :, :, :] * mask_mag + linear_mag)\n",
    "        # out_mag: (batch_size, target_sources_num, output_channels, time_steps, freq_bins)\n",
    "\n",
    "        # Calculate Y_{real} and Y_{imag} for ISTFT.\n",
    "        out_real = out_mag * out_cos\n",
    "        out_imag = out_mag * out_sin\n",
    "        # out_real, out_imag: (batch_size, target_sources_num, output_channels, time_steps, freq_bins)\n",
    "\n",
    "        # Reformat shape to (N, 1, time_steps, freq_bins) for ISTFT where\n",
    "        # N = batch_size * target_sources_num * output_channels\n",
    "        shape = (\n",
    "            batch_size * self.target_sources_num * self.output_channels,\n",
    "            1,\n",
    "            time_steps,\n",
    "            freq_bins,\n",
    "        )\n",
    "        out_real = out_real.reshape(shape)\n",
    "        out_imag = out_imag.reshape(shape)\n",
    "\n",
    "        # ISTFT.\n",
    "        x = self.istft(out_real, out_imag, audio_length)\n",
    "        # (batch_size * target_sources_num * output_channels, segments_num)\n",
    "\n",
    "        # Reshape.\n",
    "        waveform = x.reshape(\n",
    "            batch_size, self.target_sources_num * self.output_channels, audio_length\n",
    "        )\n",
    "        # (batch_size, target_sources_num * output_channels, segments_num)\n",
    "\n",
    "        return waveform\n",
    "\n",
    "    def forward(self, input_dict):\n",
    "        r\"\"\"Forward data into the module.\n",
    "\n",
    "        Args:\n",
    "            input_dict: dict, e.g., {\n",
    "                waveform: (batch_size, input_channels, segment_samples),\n",
    "                ...,\n",
    "            }\n",
    "\n",
    "        Outputs:\n",
    "            output_dict: dict, e.g., {\n",
    "                'waveform': (batch_size, output_channels, segment_samples),\n",
    "                ...,\n",
    "            }\n",
    "        \"\"\"\n",
    "        mixtures = input_dict['waveform']\n",
    "        # (batch_size, input_channels, segment_samples)\n",
    "\n",
    "        subband_x = self.pqmf.analysis(mixtures)\n",
    "        # subband_x: (batch_size, input_channels * subbands_num, segment_samples)\n",
    "\n",
    "        mag, cos_in, sin_in = self.wav_to_spectrogram_phase(subband_x)\n",
    "        # mag, cos_in, sin_in: (batch_size, input_channels * subbands_num, time_steps, freq_bins)\n",
    "\n",
    "        # Batch normalize on individual frequency bins.\n",
    "        x = mag.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        # (batch_size, input_channels * subbands_num, time_steps, freq_bins)\n",
    "\n",
    "        # Pad spectrogram to be evenly divided by downsample ratio.\n",
    "        origin_len = x.shape[2]\n",
    "        pad_len = (\n",
    "            int(np.ceil(x.shape[2] / self.time_downsample_ratio))\n",
    "            * self.time_downsample_ratio\n",
    "            - origin_len\n",
    "        )\n",
    "        x = F.pad(x, pad=(0, 0, 0, pad_len))\n",
    "        # x: (batch_size, input_channels * subbands_num, padded_time_steps, freq_bins)\n",
    "\n",
    "        # Let frequency bins be evenly divided by 2, e.g., 257 -> 256\n",
    "        x = x[..., 0 : x.shape[-1] - 1]  # (bs, input_channels, T, F)\n",
    "        # x: (batch_size, input_channels * subbands_num, padded_time_steps, freq_bins)\n",
    "\n",
    "        # UNet\n",
    "        x1_pool, x1 = self.encoder_block1(x)  # x1_pool: (bs, 32, T / 2, F / 2)\n",
    "        x2_pool, x2 = self.encoder_block2(x1_pool)  # x2_pool: (bs, 64, T / 4, F / 4)\n",
    "        x3_pool, x3 = self.encoder_block3(x2_pool)  # x3_pool: (bs, 128, T / 8, F / 8)\n",
    "        x4_pool, x4 = self.encoder_block4(x3_pool)  # x4_pool: (bs, 256, T / 16, F / 16)\n",
    "        x5_pool, x5 = self.encoder_block5(x4_pool)  # x5_pool: (bs, 384, T / 32, F / 32)\n",
    "        x6_pool, x6 = self.encoder_block6(x5_pool)  # x6_pool: (bs, 384, T / 32, F / 64)\n",
    "        x_center, _ = self.conv_block7a(x6_pool)  # (bs, 384, T / 32, F / 64)\n",
    "        x_center, _ = self.conv_block7b(x_center)  # (bs, 384, T / 32, F / 64)\n",
    "        x_center, _ = self.conv_block7c(x_center)  # (bs, 384, T / 32, F / 64)\n",
    "        x_center, _ = self.conv_block7d(x_center)  # (bs, 384, T / 32, F / 64)\n",
    "        x7 = self.decoder_block1(x_center, x6)  # (bs, 384, T / 32, F / 32)\n",
    "        x8 = self.decoder_block2(x7, x5)  # (bs, 384, T / 16, F / 16)\n",
    "        x9 = self.decoder_block3(x8, x4)  # (bs, 256, T / 8, F / 8)\n",
    "        x10 = self.decoder_block4(x9, x3)  # (bs, 128, T / 4, F / 4)\n",
    "        x11 = self.decoder_block5(x10, x2)  # (bs, 64, T / 2, F / 2)\n",
    "        x12 = self.decoder_block6(x11, x1)  # (bs, 32, T, F)\n",
    "        x, _ = self.after_conv_block1(x12)  # (bs, 32, T, F)\n",
    "\n",
    "        x = self.after_conv2(x)\n",
    "        # (batch_size, target_sources_num * output_channels * self.K * subbands_num, T, F')\n",
    "\n",
    "        # Recover shape\n",
    "        x = F.pad(x, pad=(0, 1))  # Pad frequency, e.g., 256 -> 257.\n",
    "\n",
    "        x = x[:, :, 0:origin_len, :]\n",
    "        # (batch_size, target_sources_num * output_channels * self.K * subbands_num, T, F')\n",
    "\n",
    "        audio_length = subband_x.shape[2]\n",
    "\n",
    "        # Recover each subband spectrograms to subband waveforms. Then synthesis\n",
    "        # the subband waveforms to a waveform.\n",
    "        separated_subband_audio = torch.stack(\n",
    "            [\n",
    "                self.feature_maps_to_wav(\n",
    "                    input_tensor=x[:, j :: self.subbands_num, :, :],\n",
    "                    # input_tensor: (batch_size, target_sources_num * output_channels * self.K, T, F')\n",
    "                    sp=mag[:, j :: self.subbands_num, :, :],\n",
    "                    # sp: (batch_size, input_channels, T, F')\n",
    "                    sin_in=sin_in[:, j :: self.subbands_num, :, :],\n",
    "                    # sin_in: (batch_size, input_channels, T, F')\n",
    "                    cos_in=cos_in[:, j :: self.subbands_num, :, :],\n",
    "                    # cos_in: (batch_size, input_channels, T, F')\n",
    "                    audio_length=audio_length,\n",
    "                )\n",
    "                # (batch_size, target_sources_num * output_channels, segments_num)\n",
    "                for j in range(self.subbands_num)\n",
    "            ],\n",
    "            dim=2,\n",
    "        )\n",
    "        # （batch_size, target_sources_num * output_channels, subbands_num, segment_samples)\n",
    "\n",
    "        # Format for synthesis.\n",
    "        shape = (\n",
    "            separated_subband_audio.shape[0],  # batch_size\n",
    "            self.target_sources_num * self.output_channels * self.subbands_num,\n",
    "            audio_length,\n",
    "        )\n",
    "        separated_subband_audio = separated_subband_audio.reshape(shape)\n",
    "        # （batch_size, target_sources_num * output_channels * subbands_num, segment_samples)\n",
    "\n",
    "        separated_audio = self.pqmf.synthesis(separated_subband_audio)\n",
    "        # (batch_size, input_channles, segment_samples)\n",
    "\n",
    "        output_dict = {'waveform': separated_audio}\n",
    "\n",
    "        return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "class LitSourceSeparation(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_data_preprocessor,\n",
    "        model: nn.Module,\n",
    "        loss_function: Callable,\n",
    "        optimizer_type: str,\n",
    "        learning_rate: float,\n",
    "        lr_lambda: Callable,\n",
    "    ):\n",
    "        r\"\"\"Pytorch Lightning wrapper of PyTorch model, including forward,\n",
    "        optimization of model, etc.\n",
    "\n",
    "        Args:\n",
    "            batch_data_preprocessor: object, used for preparing inputs and\n",
    "                targets for training. E.g., BasicBatchDataPreprocessor is used\n",
    "                for preparing data in dictionary into tensor.\n",
    "            model: nn.Module\n",
    "            loss_function: function\n",
    "            learning_rate: float\n",
    "            lr_lambda: function\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_data_preprocessor = batch_data_preprocessor\n",
    "        self.model = model\n",
    "        self.optimizer_type = optimizer_type\n",
    "        self.loss_function = loss_function\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_lambda = lr_lambda\n",
    "\n",
    "    def training_step(self, batch_data_dict: Dict, batch_idx: int) -> torch.float:\n",
    "        r\"\"\"Forward a mini-batch data to model, calculate loss function, and\n",
    "        train for one step. A mini-batch data is evenly distributed to multiple\n",
    "        devices (if there are) for parallel training.\n",
    "\n",
    "        Args:\n",
    "            batch_data_dict: e.g. {\n",
    "                'vocals': (batch_size, channels_num, segment_samples),\n",
    "                'accompaniment': (batch_size, channels_num, segment_samples),\n",
    "                'mixture': (batch_size, channels_num, segment_samples)\n",
    "            }\n",
    "            batch_idx: int\n",
    "\n",
    "        Returns:\n",
    "            loss: float, loss function of this mini-batch\n",
    "        \"\"\"\n",
    "        input_dict, target_dict = self.batch_data_preprocessor(batch_data_dict)\n",
    "        # input_dict: {\n",
    "        #     'waveform': (batch_size, channels_num, segment_samples),\n",
    "        #     (if_exist) 'condition': (batch_size, channels_num),\n",
    "        # }\n",
    "        # target_dict: {\n",
    "        #     'waveform': (batch_size, target_sources_num * channels_num, segment_samples),\n",
    "        # }\n",
    "\n",
    "        '''\n",
    "        import numpy as np\n",
    "        import librosa\n",
    "        import matplotlib.pyplot as plt\n",
    "        n = 1\n",
    "        in_wav = input_dict['waveform'].data.cpu().numpy()[n]\n",
    "        out_wav = target_dict['waveform'].data.cpu().numpy()[n]\n",
    "        in_sp = librosa.feature.melspectrogram(in_wav[0], sr=16000, n_fft=512, hop_length=160, n_mels=80, fmin=30, fmax=8000)\n",
    "        out_sp = librosa.feature.melspectrogram(out_wav[0], sr=16000, n_fft=512, hop_length=160, n_mels=80, fmin=30, fmax=8000)\n",
    "        out_sp2 = librosa.feature.melspectrogram(out_wav[1], sr=16000, n_fft=512, hop_length=160, n_mels=80, fmin=30, fmax=8000)\n",
    "        fig, axs = plt.subplots(3,1, sharex=True, figsize=(10, 8))\n",
    "        vmax = np.max(np.log(in_sp))\n",
    "        vmin = np.min(np.log(in_sp))\n",
    "        axs[0].matshow(np.log(in_sp), origin='lower', aspect='auto', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        axs[1].matshow(np.log(out_sp), origin='lower', aspect='auto', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        axs[2].matshow(np.log(out_sp2), origin='lower', aspect='auto', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        axs[0].grid(linestyle='solid', linewidth=0.3)\n",
    "        axs[1].grid(linestyle='solid', linewidth=0.3)\n",
    "        axs[2].grid(linestyle='solid', linewidth=0.3)\n",
    "        # axs[0].imshow(np.log(in_sp), interpolation='none')\n",
    "        # axs[1].imshow(np.log(out_sp), interpolation='none')\n",
    "        plt.savefig('_zz.pdf')\n",
    "        import soundfile\n",
    "        soundfile.write(file='_zz.wav', data=in_wav[0], samplerate=16000)\n",
    "        soundfile.write(file='_zz2.wav', data=out_wav[0], samplerate=16000)\n",
    "        from IPython import embed; embed(using=False); os._exit(0)\n",
    "        '''\n",
    "\n",
    "        # Forward.\n",
    "        self.model.train()\n",
    "\n",
    "        output_dict = self.model(input_dict)\n",
    "        # output_dict: {\n",
    "        #     'waveform': (batch_size, target_sources_num * channels_num, segment_samples),\n",
    "        # }\n",
    "\n",
    "        outputs = output_dict['waveform']\n",
    "        # outputs:, e.g, (batch_size, target_sources_num * channels_num, segment_samples)\n",
    "\n",
    "        # Calculate loss.\n",
    "        loss = self.loss_function(\n",
    "            output=outputs,\n",
    "            target=target_dict['waveform'],\n",
    "            mixture=input_dict['waveform'],\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self) -> Any:\n",
    "        r\"\"\"Configure optimizer.\"\"\"\n",
    "\n",
    "        if self.optimizer_type == \"Adam\":\n",
    "            optimizer = optim.Adam(\n",
    "                self.model.parameters(),\n",
    "                lr=self.learning_rate,\n",
    "                betas=(0.9, 0.999),\n",
    "                eps=1e-08,\n",
    "                weight_decay=0.0,\n",
    "                amsgrad=True,\n",
    "            )\n",
    "\n",
    "        elif self.optimizer_type == \"AdamW\":\n",
    "            optimizer = optim.AdamW(\n",
    "                self.model.parameters(),\n",
    "                lr=self.learning_rate,\n",
    "                betas=(0.9, 0.999),\n",
    "                eps=1e-08,\n",
    "                weight_decay=0.0,\n",
    "                amsgrad=True,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        scheduler = {\n",
    "            'scheduler': LambdaLR(optimizer, self.lr_lambda),\n",
    "            'interval': 'step',\n",
    "            'frequency': 1,\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "def get_model_class(model_type):\n",
    "    r\"\"\"Get model.\n",
    "\n",
    "    Args:\n",
    "        model_type: str, e.g., 'ResUNet143_DecouplePlusInplaceABN'\n",
    "\n",
    "    Returns:\n",
    "        nn.Module\n",
    "    \"\"\"\n",
    "    if model_type == 'ResUNet143_DecouplePlusInplaceABN_ISMIR2021':\n",
    "        # from bytesep.models.resunet_ismir2021 import (\n",
    "        #     ResUNet143_DecouplePlusInplaceABN_ISMIR2021,\n",
    "        # )\n",
    "\n",
    "        return ResUNet143_DecouplePlusInplaceABN_ISMIR2021\n",
    "\n",
    "    elif model_type == 'UNet':\n",
    "        # from bytesep.models.unet import UNet\n",
    "\n",
    "        return UNet\n",
    "\n",
    "    # TODO - add later\n",
    "    # elif model_type == 'UNetSubbandTime':\n",
    "    #     # from bytesep.models.unet_subbandtime import UNetSubbandTime\n",
    "\n",
    "    #     return UNetSubbandTime\n",
    "\n",
    "    elif model_type == 'ResUNet143_Subbandtime':\n",
    "        # from bytesep.models.resunet_subbandtime import ResUNet143_Subbandtime\n",
    "\n",
    "        return ResUNet143_Subbandtime\n",
    "\n",
    "    # TODO - add later\n",
    "    # elif model_type == 'MobileNet_Subbandtime':\n",
    "    #     # from bytesep.models.mobilenet_subbandtime import MobileNet_Subbandtime\n",
    "\n",
    "    #     return MobileNet_Subbandtime\n",
    "\n",
    "    # elif model_type == 'MobileTiny_Subbandtime':\n",
    "    #     from bytesep.models.mobiletiny_subbandtime import MobileTiny_Subbandtime\n",
    "\n",
    "    #     return MobileTiny_Subbandtime\n",
    "\n",
    "    # TODO - add later\n",
    "    # elif model_type == 'ResUNet143_DecouplePlus':\n",
    "    #     # from bytesep.models.resunet import ResUNet143_DecouplePlus\n",
    "\n",
    "    #     return ResUNet143_Dec\n",
    "\n",
    "    # TODO - add later\n",
    "    # elif model_type == 'ConditionalUNet':\n",
    "    #     # from bytesep.models.conditional_unet import ConditionalUNet\n",
    "\n",
    "    #     return ConditionalUNet\n",
    "\n",
    "    # elif model_type == 'LevelRNN':\n",
    "    #     from bytesep.models.levelrnn import LevelRNN\n",
    "\n",
    "    #     return LevelRNN\n",
    "\n",
    "    # elif model_type == 'WavUNet':\n",
    "    #     from bytesep.models.wavunet import WavUNet\n",
    "\n",
    "    #     return WavUNet\n",
    "\n",
    "    # elif model_type == 'WavUNetLevelRNN':\n",
    "    #     from bytesep.models.wavunet_levelrnn import WavUNetLevelRNN\n",
    "\n",
    "    #     return WavUNetLevelRNN\n",
    "\n",
    "    # elif model_type == 'TTnet':\n",
    "    #     from bytesep.models.ttnet import TTnet\n",
    "\n",
    "    #     return TTnet\n",
    "\n",
    "    # elif model_type == 'TTnetNoTransformer':\n",
    "    #     from bytesep.models.ttnet_no_transformer import TTnetNoTransformer\n",
    "\n",
    "    #     return TTnetNoTransformer\n",
    "\n",
    "    # elif model_type == 'JiafengCNN':\n",
    "    #     from bytesep.models.ttnet_jiafeng import JiafengCNN\n",
    "\n",
    "    #     return JiafengCNN\n",
    "\n",
    "    # elif model_type == 'JiafengTTNet':\n",
    "    #     from bytesep.models.ttnet_jiafeng import JiafengTTNet\n",
    "\n",
    "    #     return JiafengTTNet\n",
    "\n",
    "    # elif model_type == 'ResUNet143FC_Subbandtime':\n",
    "    #     from bytesep.models.resunet_subbandtime2 import ResUNet143FC_Subbandtime\n",
    "\n",
    "    #     return ResUNet143FC_Subbandtime\n",
    "\n",
    "    # elif model_type == 'AmbisonicToBinaural_UNetSubbandtimePhase':\n",
    "    #     from bytesep.models.ambisonic_to_binaural import (\n",
    "    #         AmbisonicToBinaural_UNetSubbandtimePhase,\n",
    "    #     )\n",
    "\n",
    "    #     return AmbisonicToBinaural_UNetSubbandtimePhase\n",
    "\n",
    "    # elif model_type == 'AmbisonicToBinaural_ResUNetSubbandtimePhase':\n",
    "    #     from bytesep.models.ambisonic_to_binaural import (\n",
    "    #         AmbisonicToBinaural_ResUNetSubbandtimePhase,\n",
    "    #     )\n",
    "\n",
    "    #     return AmbisonicToBinaural_ResUNetSubbandtimePhase\n",
    "\n",
    "    # TODO - add later\n",
    "    # elif model_type == 'MobileNetSubbandTime':\n",
    "    #     # from bytesep.models.mobilenet_subbandtime import MobileNetSubbandTime\n",
    "\n",
    "    #     return MobileNetSubbandTime\n",
    "\n",
    "    # elif model_type == 'WrapperDemucs':\n",
    "    #     from bytesep.models.demucs.demucs import WrapperDemucs\n",
    "\n",
    "    #     return WrapperDemucs\n",
    "\n",
    "    # elif model_type == 'WrapperHDemucs':\n",
    "    #     from bytesep.models.demucs.hdemucs import WrapperHDemucs\n",
    "\n",
    "    #     return WrapperHDemucs\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"{} not implemented!\".format(model_type))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Callable\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchlibrosa.stft import STFT\n",
    "\n",
    "# from bytesep.models.pytorch_modules import Base\n",
    "\n",
    "\n",
    "def l1(output: torch.Tensor, target: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "    r\"\"\"L1 loss.\n",
    "\n",
    "    Args:\n",
    "        output: torch.Tensor\n",
    "        target: torch.Tensor\n",
    "\n",
    "    Returns:\n",
    "        loss: torch.float\n",
    "    \"\"\"\n",
    "    return torch.mean(torch.abs(output - target))\n",
    "\n",
    "\n",
    "def l1_wav(output: torch.Tensor, target: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "    r\"\"\"L1 loss in the time-domain.\n",
    "\n",
    "    Args:\n",
    "        output: torch.Tensor\n",
    "        target: torch.Tensor\n",
    "\n",
    "    Returns:\n",
    "        loss: torch.float\n",
    "    \"\"\"\n",
    "    return l1(output, target)\n",
    "\n",
    "\n",
    "class L1_Wav_L1_Sp(nn.Module, Base):\n",
    "    def __init__(self):\n",
    "        r\"\"\"L1 loss in the time-domain and L1 loss on the spectrogram.\"\"\"\n",
    "        super(L1_Wav_L1_Sp, self).__init__()\n",
    "\n",
    "        self.window_size = 2048\n",
    "        hop_size = 441\n",
    "        center = True\n",
    "        pad_mode = \"reflect\"\n",
    "        window = \"hann\"\n",
    "\n",
    "        self.stft = STFT(\n",
    "            n_fft=self.window_size,\n",
    "            hop_length=hop_size,\n",
    "            win_length=self.window_size,\n",
    "            window=window,\n",
    "            center=center,\n",
    "            pad_mode=pad_mode,\n",
    "            freeze_parameters=True,\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self, output: torch.Tensor, target: torch.Tensor, **kwargs\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"L1 loss in the time-domain and on the spectrogram.\n",
    "\n",
    "        Args:\n",
    "            output: torch.Tensor\n",
    "            target: torch.Tensor\n",
    "\n",
    "        Returns:\n",
    "            loss: torch.float\n",
    "        \"\"\"\n",
    "\n",
    "        # L1 loss in the time-domain.\n",
    "        wav_loss = l1_wav(output, target)\n",
    "\n",
    "        # L1 loss on the spectrogram.\n",
    "        sp_loss = l1(\n",
    "            self.wav_to_spectrogram(output, eps=1e-8),\n",
    "            self.wav_to_spectrogram(target, eps=1e-8),\n",
    "        )\n",
    "\n",
    "        # sp_loss /= math.sqrt(self.window_size)\n",
    "        # sp_loss *= 1.\n",
    "\n",
    "        # Total loss.\n",
    "        return wav_loss + sp_loss\n",
    "\n",
    "        return sp_loss\n",
    "\n",
    "\n",
    "class L1_Wav_L1_CompressedSp(nn.Module, Base):\n",
    "    def __init__(self):\n",
    "        r\"\"\"L1 loss in the time-domain and L1 loss on the spectrogram.\"\"\"\n",
    "        super(L1_Wav_L1_CompressedSp, self).__init__()\n",
    "\n",
    "        self.window_size = 2048\n",
    "        hop_size = 441\n",
    "        center = True\n",
    "        pad_mode = \"reflect\"\n",
    "        window = \"hann\"\n",
    "\n",
    "        self.stft = STFT(\n",
    "            n_fft=self.window_size,\n",
    "            hop_length=hop_size,\n",
    "            win_length=self.window_size,\n",
    "            window=window,\n",
    "            center=center,\n",
    "            pad_mode=pad_mode,\n",
    "            freeze_parameters=True,\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self, output: torch.Tensor, target: torch.Tensor, **kwargs\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"L1 loss in the time-domain and on the spectrogram.\n",
    "\n",
    "        Args:\n",
    "            output: torch.Tensor\n",
    "            target: torch.Tensor\n",
    "\n",
    "        Returns:\n",
    "            loss: torch.float\n",
    "        \"\"\"\n",
    "\n",
    "        # L1 loss in the time-domain.\n",
    "        wav_loss = l1_wav(output, target)\n",
    "\n",
    "        output_mag, output_cos, output_sin = self.wav_to_spectrogram_phase(\n",
    "            output, eps=1e-8\n",
    "        )\n",
    "        target_mag, target_cos, target_sin = self.wav_to_spectrogram_phase(\n",
    "            target, eps=1e-8\n",
    "        )\n",
    "\n",
    "        mag_loss = l1(output_mag ** 0.3, target_mag ** 0.3)\n",
    "        real_loss = l1(output_mag ** 0.3 * output_cos, target_mag ** 0.3 * target_cos)\n",
    "        imag_loss = l1(output_mag ** 0.3 * output_sin, target_mag ** 0.3 * target_sin)\n",
    "\n",
    "        total_loss = wav_loss + mag_loss + real_loss + imag_loss\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "def get_loss_function(loss_type: str) -> Callable:\n",
    "    r\"\"\"Get loss function.\n",
    "\n",
    "    Args:\n",
    "        loss_type: str\n",
    "\n",
    "    Returns:\n",
    "        loss function: Callable\n",
    "    \"\"\"\n",
    "\n",
    "    if loss_type == \"l1_wav\":\n",
    "        return l1_wav\n",
    "\n",
    "    elif loss_type == \"l1_wav_l1_sp\":\n",
    "        return L1_Wav_L1_Sp()\n",
    "\n",
    "    elif loss_type == \"l1_wav_l1_compressed_sp\":\n",
    "        return L1_Wav_L1_CompressedSp()\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from typing import NoReturn\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "\n",
    "\n",
    "class SaveCheckpointsCallback(pl.Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        checkpoints_dir: str,\n",
    "        save_step_frequency: int,\n",
    "    ):\n",
    "        r\"\"\"Callback to save checkpoints every #save_step_frequency steps.\n",
    "\n",
    "        Args:\n",
    "            model: nn.Module\n",
    "            checkpoints_dir: str, directory to save checkpoints\n",
    "            save_step_frequency: int\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.checkpoints_dir = checkpoints_dir\n",
    "        self.save_step_frequency = save_step_frequency\n",
    "        os.makedirs(self.checkpoints_dir, exist_ok=True)\n",
    "\n",
    "    @rank_zero_only\n",
    "    def on_batch_end(self, trainer: pl.Trainer, _) -> NoReturn:\n",
    "        r\"\"\"Save checkpoint.\"\"\"\n",
    "        global_step = trainer.global_step\n",
    "\n",
    "        if global_step % self.save_step_frequency == 0:\n",
    "\n",
    "            checkpoint_path = os.path.join(\n",
    "                self.checkpoints_dir, \"step={}.pth\".format(global_step)\n",
    "            )\n",
    "\n",
    "            checkpoint = {'step': global_step, 'model': self.model.state_dict()}\n",
    "\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            logging.info(\"Save checkpoint to {}\".format(checkpoint_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_audio(\n",
    "    audio: np.array, mono: bool, origin_sr: float, sr: float, resample_type: str\n",
    ") -> np.array:\n",
    "    r\"\"\"Preprocess audio to mono / stereo, and resample.\n",
    "\n",
    "    Args:\n",
    "        audio: (channels_num, audio_samples), input audio\n",
    "        mono: bool\n",
    "        origin_sr: float, original sample rate\n",
    "        sr: float, target sample rate\n",
    "        resample_type: str, e.g., 'kaiser_fast'\n",
    "\n",
    "    Returns:\n",
    "        output: ndarray, output audio\n",
    "    \"\"\"\n",
    "    if mono:\n",
    "        audio = np.mean(audio, axis=0)\n",
    "        # (audio_samples,)\n",
    "\n",
    "    output = librosa.core.resample(\n",
    "        audio, orig_sr=origin_sr, target_sr=sr, res_type=resample_type\n",
    "    )\n",
    "    # (audio_samples,) | (channels_num, audio_samples)\n",
    "\n",
    "    if output.ndim == 1:\n",
    "        output = output[None, :]\n",
    "        # (1, audio_samples,)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Separator:\n",
    "    def __init__(\n",
    "        self, model: nn.Module, segment_samples: int, batch_size: int, device: str\n",
    "    ):\n",
    "        r\"\"\"Separate to separate an audio clip into a target source.\n",
    "\n",
    "        Args:\n",
    "            model: nn.Module, trained model\n",
    "            segment_samples: int, length of segments to be input to a model, e.g., 44100*30\n",
    "            batch_size, int, e.g., 12\n",
    "            device: str, e.g., 'cuda'\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.segment_samples = segment_samples\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "    def separate(self, input_dict: Dict) -> np.array:\n",
    "        r\"\"\"Separate an audio clip into a target source.\n",
    "\n",
    "        Args:\n",
    "            input_dict: dict, e.g., {\n",
    "                waveform: (channels_num, audio_samples),\n",
    "                ...,\n",
    "            }\n",
    "\n",
    "        Returns:\n",
    "            sep_audio: (channels_num, audio_samples) | (target_sources_num, channels_num, audio_samples)\n",
    "        \"\"\"\n",
    "        audio = input_dict['waveform']\n",
    "\n",
    "        audio_samples = audio.shape[-1]\n",
    "\n",
    "        # Pad the audio with zero in the end so that the length of audio can be\n",
    "        # evenly divided by segment_samples.\n",
    "        audio = self.pad_audio(audio)\n",
    "\n",
    "        # Enframe long audio into segments.\n",
    "        segments = self.enframe(audio, self.segment_samples)\n",
    "        # (segments_num, channels_num, segment_samples)\n",
    "\n",
    "        segments_input_dict = {'waveform': segments}\n",
    "\n",
    "        if 'condition' in input_dict.keys():\n",
    "            segments_num = len(segments)\n",
    "            segments_input_dict['condition'] = np.tile(\n",
    "                input_dict['condition'][None, :], (segments_num, 1)\n",
    "            )\n",
    "            # (batch_size, segments_num)\n",
    "\n",
    "        # Separate in mini-batches.\n",
    "        sep_segments = self._forward_in_mini_batches(\n",
    "            self.model, segments_input_dict, self.batch_size\n",
    "        )['waveform']\n",
    "        # (segments_num, channels_num, segment_samples)\n",
    "\n",
    "        # Deframe segments into long audio.\n",
    "        sep_audio = self.deframe(sep_segments)\n",
    "        # (channels_num, padded_audio_samples)\n",
    "\n",
    "        sep_audio = sep_audio[:, 0:audio_samples]\n",
    "        # (channels_num, audio_samples)\n",
    "\n",
    "        return sep_audio\n",
    "\n",
    "    def pad_audio(self, audio: np.array) -> np.array:\n",
    "        r\"\"\"Pad the audio with zero in the end so that the length of audio can\n",
    "        be evenly divided by segment_samples.\n",
    "\n",
    "        Args:\n",
    "            audio: (channels_num, audio_samples)\n",
    "\n",
    "        Returns:\n",
    "            padded_audio: (channels_num, audio_samples)\n",
    "        \"\"\"\n",
    "        channels_num, audio_samples = audio.shape\n",
    "\n",
    "        # Number of segments\n",
    "        segments_num = int(np.ceil(audio_samples / self.segment_samples))\n",
    "\n",
    "        pad_samples = segments_num * self.segment_samples - audio_samples\n",
    "\n",
    "        padded_audio = np.concatenate(\n",
    "            (audio, np.zeros((channels_num, pad_samples))), axis=1\n",
    "        )\n",
    "        # (channels_num, padded_audio_samples)\n",
    "\n",
    "        return padded_audio\n",
    "\n",
    "    def enframe(self, audio: np.array, segment_samples: int) -> np.array:\n",
    "        r\"\"\"Enframe long audio into segments.\n",
    "\n",
    "        Args:\n",
    "            audio: (channels_num, audio_samples)\n",
    "            segment_samples: int\n",
    "\n",
    "        Returns:\n",
    "            segments: (segments_num, channels_num, segment_samples)\n",
    "        \"\"\"\n",
    "        audio_samples = audio.shape[1]\n",
    "        assert audio_samples % segment_samples == 0\n",
    "\n",
    "        hop_samples = segment_samples // 2\n",
    "        segments = []\n",
    "\n",
    "        pointer = 0\n",
    "        while pointer + segment_samples <= audio_samples:\n",
    "            segments.append(audio[:, pointer : pointer + segment_samples])\n",
    "            pointer += hop_samples\n",
    "\n",
    "        segments = np.array(segments)\n",
    "\n",
    "        return segments\n",
    "\n",
    "    def deframe(self, segments: np.array) -> np.array:\n",
    "        r\"\"\"Deframe segments into long audio.\n",
    "\n",
    "        Args:\n",
    "            segments: (segments_num, channels_num, segment_samples)\n",
    "\n",
    "        Returns:\n",
    "            output: (channels_num, audio_samples)\n",
    "        \"\"\"\n",
    "        (segments_num, _, segment_samples) = segments.shape\n",
    "\n",
    "        if segments_num == 1:\n",
    "            return segments[0]\n",
    "\n",
    "        assert self._is_integer(segment_samples * 0.25)\n",
    "        assert self._is_integer(segment_samples * 0.75)\n",
    "\n",
    "        output = []\n",
    "\n",
    "        output.append(segments[0, :, 0 : int(segment_samples * 0.75)])\n",
    "\n",
    "        for i in range(1, segments_num - 1):\n",
    "            output.append(\n",
    "                segments[\n",
    "                    i, :, int(segment_samples * 0.25) : int(segment_samples * 0.75)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        output.append(segments[-1, :, int(segment_samples * 0.25) :])\n",
    "\n",
    "        output = np.concatenate(output, axis=-1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _is_integer(self, x: float) -> bool:\n",
    "        if x - int(x) < 1e-10:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _forward_in_mini_batches(\n",
    "        self, model: nn.Module, segments_input_dict: Dict, batch_size: int\n",
    "    ) -> Dict:\n",
    "        r\"\"\"Forward data to model in mini-batch.\n",
    "\n",
    "        Args:\n",
    "            model: nn.Module\n",
    "            segments_input_dict: dict, e.g., {\n",
    "                'waveform': (segments_num, channels_num, segment_samples),\n",
    "                ...,\n",
    "            }\n",
    "            batch_size: int\n",
    "\n",
    "        Returns:\n",
    "            output_dict: dict, e.g. {\n",
    "                'waveform': (segments_num, channels_num, segment_samples),\n",
    "            }\n",
    "        \"\"\"\n",
    "        output_dict = {}\n",
    "\n",
    "        pointer = 0\n",
    "        segments_num = len(segments_input_dict['waveform'])\n",
    "\n",
    "        while True:\n",
    "            if pointer >= segments_num:\n",
    "                break\n",
    "\n",
    "            batch_input_dict = {}\n",
    "\n",
    "            for key in segments_input_dict.keys():\n",
    "                batch_input_dict[key] = torch.Tensor(\n",
    "                    segments_input_dict[key][pointer : pointer + batch_size]\n",
    "                ).to(self.device)\n",
    "\n",
    "            pointer += batch_size\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                batch_output_dict = model(batch_input_dict)\n",
    "\n",
    "            for key in batch_output_dict.keys():\n",
    "                self._append_to_dict(\n",
    "                    output_dict, key, batch_output_dict[key].data.cpu().numpy()\n",
    "                )\n",
    "\n",
    "        for key in output_dict.keys():\n",
    "            output_dict[key] = np.concatenate(output_dict[key], axis=0)\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    def _append_to_dict(self, dict, key, value):\n",
    "        if key in dict.keys():\n",
    "            dict[key].append(value)\n",
    "        else:\n",
    "            dict[key] = [value]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, List, NoReturn\n",
    "\n",
    "import librosa\n",
    "import musdb\n",
    "import museval\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "\n",
    "# from bytesep.callbacks.base import SaveCheckpointsCallback\n",
    "# from bytesep.dataset_creation.pack_audios_to_hdf5s.musdb18 import preprocess_audio\n",
    "# from bytesep.separate import Separator\n",
    "# from bytesep.utils import StatisticsContainer, read_yaml\n",
    "\n",
    "\n",
    "def get_musdb18_callbacks(\n",
    "    config_yaml: str,\n",
    "    workspace: str,\n",
    "    checkpoints_dir: str,\n",
    "    statistics_path: str,\n",
    "    logger: pl.loggers.TensorBoardLogger,\n",
    "    model: nn.Module,\n",
    "    evaluate_device: str,\n",
    ") -> List[pl.Callback]:\n",
    "    r\"\"\"Get MUSDB18 callbacks of a config yaml.\n",
    "\n",
    "    Args:\n",
    "        config_yaml: str\n",
    "        workspace: str\n",
    "        checkpoints_dir: str, directory to save checkpoints\n",
    "        statistics_dir: str, directory to save statistics\n",
    "        logger: pl.loggers.TensorBoardLogger\n",
    "        model: nn.Module\n",
    "        evaluate_device: str\n",
    "\n",
    "    Return:\n",
    "        callbacks: List[pl.Callback]\n",
    "    \"\"\"\n",
    "    configs = read_yaml(config_yaml)\n",
    "    task_name = configs['task_name']\n",
    "    evaluation_callback = configs['train']['evaluation_callback']\n",
    "    target_source_types = configs['train']['target_source_types']\n",
    "    input_channels = configs['train']['input_channels']\n",
    "    evaluation_audios_dir = os.path.join(workspace, \"evaluation_audios\", task_name)\n",
    "    test_segment_seconds = configs['evaluate']['segment_seconds']\n",
    "    sample_rate = configs['train']['sample_rate']\n",
    "    test_segment_samples = int(test_segment_seconds * sample_rate)\n",
    "    test_batch_size = configs['evaluate']['batch_size']\n",
    "\n",
    "    evaluate_step_frequency = configs['train']['evaluate_step_frequency']\n",
    "    save_step_frequency = configs['train']['save_step_frequency']\n",
    "\n",
    "    # save checkpoint callback\n",
    "    save_checkpoints_callback = SaveCheckpointsCallback(\n",
    "        model=model,\n",
    "        checkpoints_dir=checkpoints_dir,\n",
    "        save_step_frequency=save_step_frequency,\n",
    "    )\n",
    "\n",
    "    # evaluation callback\n",
    "    EvaluationCallback = _get_evaluation_callback_class(evaluation_callback)\n",
    "\n",
    "    # statistics container\n",
    "    statistics_container = StatisticsContainer(statistics_path)\n",
    "\n",
    "    # evaluation callback\n",
    "    evaluate_train_callback = EvaluationCallback(\n",
    "        dataset_dir=evaluation_audios_dir,\n",
    "        split='train',\n",
    "        model=model,\n",
    "        target_source_types=target_source_types,\n",
    "        sample_rate=sample_rate,\n",
    "        input_channels=input_channels,\n",
    "        segment_samples=test_segment_samples,\n",
    "        batch_size=test_batch_size,\n",
    "        device=evaluate_device,\n",
    "        evaluate_step_frequency=evaluate_step_frequency,\n",
    "        logger=logger,\n",
    "        statistics_container=statistics_container,\n",
    "    )\n",
    "\n",
    "    evaluate_test_callback = EvaluationCallback(\n",
    "        dataset_dir=evaluation_audios_dir,\n",
    "        split='test',\n",
    "        model=model,\n",
    "        target_source_types=target_source_types,\n",
    "        sample_rate=sample_rate,\n",
    "        input_channels=input_channels,\n",
    "        segment_samples=test_segment_samples,\n",
    "        batch_size=test_batch_size,\n",
    "        device=evaluate_device,\n",
    "        evaluate_step_frequency=evaluate_step_frequency,\n",
    "        logger=logger,\n",
    "        statistics_container=statistics_container,\n",
    "    )\n",
    "\n",
    "    # callbacks = [save_checkpoints_callback, evaluate_train_callback, evaluate_test_callback]\n",
    "    callbacks = [save_checkpoints_callback, evaluate_test_callback]\n",
    "\n",
    "    return callbacks\n",
    "\n",
    "\n",
    "def _get_evaluation_callback_class(evaluation_callback) -> pl.Callback:\n",
    "    r\"\"\"Get evaluation callback class.\"\"\"\n",
    "    if evaluation_callback == \"Musdb18\":\n",
    "        return Musdb18EvaluationCallback\n",
    "\n",
    "    if evaluation_callback == 'Musdb18Conditional':\n",
    "        return Musdb18ConditionalEvaluationCallback\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Musdb18EvaluationCallback(pl.Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_dir: str,\n",
    "        split: str,\n",
    "        model: nn.Module,\n",
    "        target_source_types: str,\n",
    "        sample_rate: int,\n",
    "        input_channels: int,\n",
    "        segment_samples: int,\n",
    "        batch_size: int,\n",
    "        device: str,\n",
    "        evaluate_step_frequency: int,\n",
    "        logger: pl.loggers.TensorBoardLogger,\n",
    "        statistics_container: StatisticsContainer,\n",
    "    ):\n",
    "        r\"\"\"Callback to evaluate every #save_step_frequency steps.\n",
    "\n",
    "        Args:\n",
    "            dataset_dir: str\n",
    "            model: nn.Module\n",
    "            target_source_types: List[str], e.g., ['vocals', 'bass', ...]\n",
    "            input_channels: int\n",
    "            split: 'train' | 'test'\n",
    "            sample_rate: int\n",
    "            segment_samples: int, length of segments to be input to a model, e.g., 44100*30\n",
    "            batch_size, int, e.g., 12\n",
    "            device: str, e.g., 'cuda'\n",
    "            evaluate_step_frequency: int, evaluate every #save_step_frequency steps\n",
    "            logger: object\n",
    "            statistics_container: StatisticsContainer\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.target_source_types = target_source_types\n",
    "        self.input_channels = input_channels\n",
    "        self.sample_rate = sample_rate\n",
    "        self.split = split\n",
    "        self.segment_samples = segment_samples\n",
    "        self.evaluate_step_frequency = evaluate_step_frequency\n",
    "        self.logger = logger\n",
    "        self.statistics_container = statistics_container\n",
    "        self.mono = input_channels == 1\n",
    "        self.resample_type = \"kaiser_fast\"\n",
    "\n",
    "        self.mus = musdb.DB(root=dataset_dir, subsets=[split])\n",
    "\n",
    "        error_msg = \"The directory {} is empty!\".format(dataset_dir)\n",
    "        assert len(self.mus) > 0, error_msg\n",
    "\n",
    "        # separator\n",
    "        self.separator = Separator(model, self.segment_samples, batch_size, device)\n",
    "\n",
    "    @rank_zero_only\n",
    "    def on_batch_end(self, trainer: pl.Trainer, _) -> NoReturn:\n",
    "        r\"\"\"Evaluate separation SDRs of audio recordings.\"\"\"\n",
    "        global_step = trainer.global_step\n",
    "\n",
    "        if global_step % self.evaluate_step_frequency == 0:\n",
    "\n",
    "            sdr_dict = {}\n",
    "\n",
    "            logging.info(\"--- Step {} ---\".format(global_step))\n",
    "            logging.info(\"Total {} pieces for evaluation:\".format(len(self.mus.tracks)))\n",
    "\n",
    "            eval_time = time.time()\n",
    "\n",
    "            for track in self.mus.tracks:\n",
    "\n",
    "                audio_name = track.name\n",
    "\n",
    "                # Get waveform of mixture.\n",
    "                mixture = track.audio.T\n",
    "                # (channels_num, audio_samples)\n",
    "\n",
    "                mixture = preprocess_audio(\n",
    "                    audio=mixture,\n",
    "                    mono=self.mono,\n",
    "                    origin_sr=track.rate,\n",
    "                    sr=self.sample_rate,\n",
    "                    resample_type=self.resample_type,\n",
    "                )\n",
    "                # (channels_num, audio_samples)\n",
    "\n",
    "                target_dict = {}\n",
    "                sdr_dict[audio_name] = {}\n",
    "\n",
    "                # Get waveform of all target source types.\n",
    "                for j, source_type in enumerate(self.target_source_types):\n",
    "                    # E.g., ['vocals', 'bass', ...]\n",
    "\n",
    "                    audio = track.targets[source_type].audio.T\n",
    "\n",
    "                    audio = preprocess_audio(\n",
    "                        audio=audio,\n",
    "                        mono=self.mono,\n",
    "                        origin_sr=track.rate,\n",
    "                        sr=self.sample_rate,\n",
    "                        resample_type=self.resample_type,\n",
    "                    )\n",
    "                    # (channels_num, audio_samples)\n",
    "\n",
    "                    target_dict[source_type] = audio\n",
    "                    # (channels_num, audio_samples)\n",
    "\n",
    "                # Separate.\n",
    "                input_dict = {'waveform': mixture}\n",
    "\n",
    "                sep_wavs = self.separator.separate(input_dict)\n",
    "                # sep_wavs: (target_sources_num * channels_num, audio_samples)\n",
    "\n",
    "                # Post process separation results.\n",
    "                sep_wavs = preprocess_audio(\n",
    "                    audio=sep_wavs,\n",
    "                    mono=self.mono,\n",
    "                    origin_sr=self.sample_rate,\n",
    "                    sr=track.rate,\n",
    "                    resample_type=self.resample_type,\n",
    "                )\n",
    "                # sep_wavs: (target_sources_num * channels_num, audio_samples)\n",
    "\n",
    "                sep_wavs = librosa.util.fix_length(\n",
    "                    sep_wavs, size=mixture.shape[1], axis=1\n",
    "                )\n",
    "                # sep_wavs: (target_sources_num * channels_num, audio_samples)\n",
    "\n",
    "                sep_wav_dict = get_separated_wavs_from_simo_output(\n",
    "                    sep_wavs, self.input_channels, self.target_source_types\n",
    "                )\n",
    "                # output_dict: dict, e.g., {\n",
    "                #     'vocals': (channels_num, audio_samples),\n",
    "                #     'bass': (channels_num, audio_samples),\n",
    "                #     ...,\n",
    "                # }\n",
    "\n",
    "                # Evaluate for all target source types.\n",
    "                for source_type in self.target_source_types:\n",
    "                    # E.g., ['vocals', 'bass', ...]\n",
    "\n",
    "                    # Calculate SDR using museval, input shape should be: (nsrc, nsampl, nchan).\n",
    "                    (sdrs, _, _, _) = museval.evaluate(\n",
    "                        [target_dict[source_type].T], [sep_wav_dict[source_type].T]\n",
    "                    )\n",
    "\n",
    "                    sdr = np.nanmedian(sdrs)\n",
    "                    sdr_dict[audio_name][source_type] = sdr\n",
    "\n",
    "                    logging.info(\n",
    "                        \"{}, {}, sdr: {:.3f}\".format(audio_name, source_type, sdr)\n",
    "                    )\n",
    "\n",
    "            logging.info(\"-----------------------------\")\n",
    "            median_sdr_dict = {}\n",
    "\n",
    "            # Calculate median SDRs of all songs.\n",
    "            for source_type in self.target_source_types:\n",
    "                # E.g., ['vocals', 'bass', ...]\n",
    "\n",
    "                median_sdr = np.median(\n",
    "                    [\n",
    "                        sdr_dict[audio_name][source_type]\n",
    "                        for audio_name in sdr_dict.keys()\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                median_sdr_dict[source_type] = median_sdr\n",
    "\n",
    "                logging.info(\n",
    "                    \"Step: {}, {}, Median SDR: {:.3f}\".format(\n",
    "                        global_step, source_type, median_sdr\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            logging.info(\"Evlauation time: {:.3f}\".format(time.time() - eval_time))\n",
    "\n",
    "            statistics = {\"sdr_dict\": sdr_dict, \"median_sdr_dict\": median_sdr_dict}\n",
    "            self.statistics_container.append(global_step, statistics, self.split)\n",
    "            self.statistics_container.dump()\n",
    "\n",
    "\n",
    "def get_separated_wavs_from_simo_output(x, input_channels, target_source_types) -> Dict:\n",
    "    r\"\"\"Get separated waveforms of target sources from a single input multiple\n",
    "    output (SIMO) system.\n",
    "\n",
    "    Args:\n",
    "        x: (target_sources_num * channels_num, audio_samples)\n",
    "        input_channels: int\n",
    "        target_source_types: List[str], e.g., ['vocals', 'bass', ...]\n",
    "\n",
    "    Returns:\n",
    "        output_dict: dict, e.g., {\n",
    "            'vocals': (channels_num, audio_samples),\n",
    "            'bass': (channels_num, audio_samples),\n",
    "            ...,\n",
    "        }\n",
    "    \"\"\"\n",
    "    output_dict = {}\n",
    "\n",
    "    for j, source_type in enumerate(target_source_types):\n",
    "        output_dict[source_type] = x[j * input_channels : (j + 1) * input_channels]\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "class Musdb18ConditionalEvaluationCallback(pl.Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_dir: str,\n",
    "        split: str,\n",
    "        model: nn.Module,\n",
    "        target_source_types: str,\n",
    "        sample_rate: int,\n",
    "        input_channels: int,\n",
    "        segment_samples: int,\n",
    "        batch_size: int,\n",
    "        device: str,\n",
    "        evaluate_step_frequency: int,\n",
    "        logger: pl.loggers.TensorBoardLogger,\n",
    "        statistics_container: StatisticsContainer,\n",
    "    ):\n",
    "        r\"\"\"Callback to evaluate every #save_step_frequency steps.\n",
    "\n",
    "        Args:\n",
    "            dataset_dir: str\n",
    "            split: 'train' | 'test'\n",
    "            model: nn.Module\n",
    "            target_source_types: List[str], e.g., ['vocals', 'bass', ...]\n",
    "            sample_rate: int\n",
    "            input_channels: int\n",
    "            segment_samples: int, length of segments to be input to a model, e.g., 44100*30\n",
    "            batch_size, int, e.g., 12\n",
    "            device: str, e.g., 'cuda'\n",
    "            evaluate_step_frequency: int, evaluate every #save_step_frequency steps\n",
    "            logger: object\n",
    "            statistics_container: StatisticsContainer\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.target_source_types = target_source_types\n",
    "        self.input_channels = input_channels\n",
    "        self.sample_rate = sample_rate\n",
    "        self.split = split\n",
    "        self.segment_samples = segment_samples\n",
    "        self.evaluate_step_frequency = evaluate_step_frequency\n",
    "        self.logger = logger\n",
    "        self.statistics_container = statistics_container\n",
    "        self.mono = input_channels == 1\n",
    "        self.resample_type = \"kaiser_fast\"\n",
    "\n",
    "        self.mus = musdb.DB(root=dataset_dir, subsets=[split])\n",
    "\n",
    "        error_msg = \"The directory {} is empty!\".format(dataset_dir)\n",
    "        assert len(self.mus) > 0, error_msg\n",
    "\n",
    "        # separator\n",
    "        self.separator = Separator(model, self.segment_samples, batch_size, device)\n",
    "\n",
    "    @rank_zero_only\n",
    "    def on_batch_end(self, trainer: pl.Trainer, _) -> NoReturn:\n",
    "        r\"\"\"Evaluate separation SDRs of audio recordings.\"\"\"\n",
    "        global_step = trainer.global_step\n",
    "\n",
    "        if global_step % self.evaluate_step_frequency == 0:\n",
    "\n",
    "            sdr_dict = {}\n",
    "\n",
    "            logging.info(\"--- Step {} ---\".format(global_step))\n",
    "            logging.info(\"Total {} pieces for evaluation:\".format(len(self.mus.tracks)))\n",
    "\n",
    "            eval_time = time.time()\n",
    "\n",
    "            for track in self.mus.tracks:\n",
    "\n",
    "                audio_name = track.name\n",
    "\n",
    "                # Get waveform of mixture.\n",
    "                mixture = track.audio.T\n",
    "                # (channels_num, audio_samples)\n",
    "\n",
    "                mixture = preprocess_audio(\n",
    "                    audio=mixture,\n",
    "                    mono=self.mono,\n",
    "                    origin_sr=track.rate,\n",
    "                    sr=self.sample_rate,\n",
    "                    resample_type=self.resample_type,\n",
    "                )\n",
    "                # (channels_num, audio_samples)\n",
    "\n",
    "                target_dict = {}\n",
    "                sdr_dict[audio_name] = {}\n",
    "\n",
    "                # Get waveform of all target source types.\n",
    "                for j, source_type in enumerate(self.target_source_types):\n",
    "                    # E.g., ['vocals', 'bass', ...]\n",
    "\n",
    "                    audio = track.targets[source_type].audio.T\n",
    "\n",
    "                    audio = preprocess_audio(\n",
    "                        audio=audio,\n",
    "                        mono=self.mono,\n",
    "                        origin_sr=track.rate,\n",
    "                        sr=self.sample_rate,\n",
    "                        resample_type=self.resample_type,\n",
    "                    )\n",
    "                    # (channels_num, audio_samples)\n",
    "\n",
    "                    target_dict[source_type] = audio\n",
    "                    # (channels_num, audio_samples)\n",
    "\n",
    "                    condition = np.zeros(len(self.target_source_types))\n",
    "                    condition[j] = 1\n",
    "\n",
    "                    input_dict = {'waveform': mixture, 'condition': condition}\n",
    "\n",
    "                    sep_wav = self.separator.separate(input_dict)\n",
    "                    # sep_wav: (channels_num, audio_samples)\n",
    "\n",
    "                    sep_wav = preprocess_audio(\n",
    "                        audio=sep_wav,\n",
    "                        mono=self.mono,\n",
    "                        origin_sr=self.sample_rate,\n",
    "                        sr=track.rate,\n",
    "                        resample_type=self.resample_type,\n",
    "                    )\n",
    "                    # sep_wav: (channels_num, audio_samples)\n",
    "\n",
    "                    sep_wav = librosa.util.fix_length(\n",
    "                        sep_wav, size=mixture.shape[1], axis=1\n",
    "                    )\n",
    "                    # sep_wav: (target_sources_num * channels_num, audio_samples)\n",
    "\n",
    "                    # Calculate SDR using museval, input shape should be: (nsrc, nsampl, nchan)\n",
    "                    (sdrs, _, _, _) = museval.evaluate(\n",
    "                        [target_dict[source_type].T], [sep_wav.T]\n",
    "                    )\n",
    "\n",
    "                    sdr = np.nanmedian(sdrs)\n",
    "                    sdr_dict[audio_name][source_type] = sdr\n",
    "\n",
    "                    logging.info(\n",
    "                        \"{}, {}, sdr: {:.3f}\".format(audio_name, source_type, sdr)\n",
    "                    )\n",
    "\n",
    "            logging.info(\"-----------------------------\")\n",
    "            median_sdr_dict = {}\n",
    "\n",
    "            # Calculate median SDRs of all songs.\n",
    "            for source_type in self.target_source_types:\n",
    "\n",
    "                median_sdr = np.median(\n",
    "                    [\n",
    "                        sdr_dict[audio_name][source_type]\n",
    "                        for audio_name in sdr_dict.keys()\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                median_sdr_dict[source_type] = median_sdr\n",
    "\n",
    "                logging.info(\n",
    "                    \"Step: {}, {}, Median SDR: {:.3f}\".format(\n",
    "                        global_step, source_type, median_sdr\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            logging.info(\"Evlauation time: {:.3f}\".format(time.time() - eval_time))\n",
    "\n",
    "            statistics = {\"sdr_dict\": sdr_dict, \"median_sdr_dict\": median_sdr_dict}\n",
    "            self.statistics_container.append(global_step, statistics, self.split)\n",
    "            self.statistics_container.dump()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def get_callbacks(\n",
    "    task_name: str,\n",
    "    config_yaml: str,\n",
    "    workspace: str,\n",
    "    checkpoints_dir: str,\n",
    "    statistics_path: str,\n",
    "    logger: pl.loggers.TensorBoardLogger,\n",
    "    model: nn.Module,\n",
    "    evaluate_device: str,\n",
    ") -> List[pl.Callback]:\n",
    "    r\"\"\"Get callbacks of a task and config yaml file.\n",
    "\n",
    "    Args:\n",
    "        task_name: str\n",
    "        config_yaml: str\n",
    "        dataset_dir: str\n",
    "        workspace: str, containing useful files such as audios for evaluation\n",
    "        checkpoints_dir: str, directory to save checkpoints\n",
    "        statistics_dir: str, directory to save statistics\n",
    "        logger: pl.loggers.TensorBoardLogger\n",
    "        model: nn.Module\n",
    "        evaluate_device: str\n",
    "\n",
    "    Return:\n",
    "        callbacks: List[pl.Callback]\n",
    "    \"\"\"\n",
    "    if task_name == 'musdb18':\n",
    "\n",
    "        # from bytesep.callbacks.musdb18 import get_musdb18_callbacks\n",
    "\n",
    "        return get_musdb18_callbacks(\n",
    "            config_yaml=config_yaml,\n",
    "            workspace=workspace,\n",
    "            checkpoints_dir=checkpoints_dir,\n",
    "            statistics_path=statistics_path,\n",
    "            logger=logger,\n",
    "            model=model,\n",
    "            evaluate_device=evaluate_device,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LitSourceSeperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "class LitSourceSeparation(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_data_preprocessor,\n",
    "        model: nn.Module,\n",
    "        loss_function: Callable,\n",
    "        optimizer_type: str,\n",
    "        learning_rate: float,\n",
    "        lr_lambda: Callable,\n",
    "    ):\n",
    "        r\"\"\"Pytorch Lightning wrapper of PyTorch model, including forward,\n",
    "        optimization of model, etc.\n",
    "\n",
    "        Args:\n",
    "            batch_data_preprocessor: object, used for preparing inputs and\n",
    "                targets for training. E.g., BasicBatchDataPreprocessor is used\n",
    "                for preparing data in dictionary into tensor.\n",
    "            model: nn.Module\n",
    "            loss_function: function\n",
    "            learning_rate: float\n",
    "            lr_lambda: function\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_data_preprocessor = batch_data_preprocessor\n",
    "        self.model = model\n",
    "        self.optimizer_type = optimizer_type\n",
    "        self.loss_function = loss_function\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_lambda = lr_lambda\n",
    "\n",
    "    def training_step(self, batch_data_dict: Dict, batch_idx: int) -> torch.float:\n",
    "        r\"\"\"Forward a mini-batch data to model, calculate loss function, and\n",
    "        train for one step. A mini-batch data is evenly distributed to multiple\n",
    "        devices (if there are) for parallel training.\n",
    "\n",
    "        Args:\n",
    "            batch_data_dict: e.g. {\n",
    "                'vocals': (batch_size, channels_num, segment_samples),\n",
    "                'accompaniment': (batch_size, channels_num, segment_samples),\n",
    "                'mixture': (batch_size, channels_num, segment_samples)\n",
    "            }\n",
    "            batch_idx: int\n",
    "\n",
    "        Returns:\n",
    "            loss: float, loss function of this mini-batch\n",
    "        \"\"\"\n",
    "        input_dict, target_dict = self.batch_data_preprocessor(batch_data_dict)\n",
    "        # input_dict: {\n",
    "        #     'waveform': (batch_size, channels_num, segment_samples),\n",
    "        #     (if_exist) 'condition': (batch_size, channels_num),\n",
    "        # }\n",
    "        # target_dict: {\n",
    "        #     'waveform': (batch_size, target_sources_num * channels_num, segment_samples),\n",
    "        # }\n",
    "\n",
    "        '''\n",
    "        import numpy as np\n",
    "        import librosa\n",
    "        import matplotlib.pyplot as plt\n",
    "        n = 1\n",
    "        in_wav = input_dict['waveform'].data.cpu().numpy()[n]\n",
    "        out_wav = target_dict['waveform'].data.cpu().numpy()[n]\n",
    "        in_sp = librosa.feature.melspectrogram(in_wav[0], sr=16000, n_fft=512, hop_length=160, n_mels=80, fmin=30, fmax=8000)\n",
    "        out_sp = librosa.feature.melspectrogram(out_wav[0], sr=16000, n_fft=512, hop_length=160, n_mels=80, fmin=30, fmax=8000)\n",
    "        out_sp2 = librosa.feature.melspectrogram(out_wav[1], sr=16000, n_fft=512, hop_length=160, n_mels=80, fmin=30, fmax=8000)\n",
    "        fig, axs = plt.subplots(3,1, sharex=True, figsize=(10, 8))\n",
    "        vmax = np.max(np.log(in_sp))\n",
    "        vmin = np.min(np.log(in_sp))\n",
    "        axs[0].matshow(np.log(in_sp), origin='lower', aspect='auto', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        axs[1].matshow(np.log(out_sp), origin='lower', aspect='auto', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        axs[2].matshow(np.log(out_sp2), origin='lower', aspect='auto', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        axs[0].grid(linestyle='solid', linewidth=0.3)\n",
    "        axs[1].grid(linestyle='solid', linewidth=0.3)\n",
    "        axs[2].grid(linestyle='solid', linewidth=0.3)\n",
    "        # axs[0].imshow(np.log(in_sp), interpolation='none')\n",
    "        # axs[1].imshow(np.log(out_sp), interpolation='none')\n",
    "        plt.savefig('_zz.pdf')\n",
    "        import soundfile\n",
    "        soundfile.write(file='_zz.wav', data=in_wav[0], samplerate=16000)\n",
    "        soundfile.write(file='_zz2.wav', data=out_wav[0], samplerate=16000)\n",
    "        from IPython import embed; embed(using=False); os._exit(0)\n",
    "        '''\n",
    "\n",
    "        # Forward.\n",
    "        self.model.train()\n",
    "\n",
    "        output_dict = self.model(input_dict)\n",
    "        # output_dict: {\n",
    "        #     'waveform': (batch_size, target_sources_num * channels_num, segment_samples),\n",
    "        # }\n",
    "\n",
    "        outputs = output_dict['waveform']\n",
    "        # outputs:, e.g, (batch_size, target_sources_num * channels_num, segment_samples)\n",
    "\n",
    "        # Calculate loss.\n",
    "        loss = self.loss_function(\n",
    "            output=outputs,\n",
    "            target=target_dict['waveform'],\n",
    "            mixture=input_dict['waveform'],\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self) -> Any:\n",
    "        r\"\"\"Configure optimizer.\"\"\"\n",
    "\n",
    "        if self.optimizer_type == \"Adam\":\n",
    "            optimizer = optim.Adam(\n",
    "                self.model.parameters(),\n",
    "                lr=self.learning_rate,\n",
    "                betas=(0.9, 0.999),\n",
    "                eps=1e-08,\n",
    "                weight_decay=0.0,\n",
    "                amsgrad=True,\n",
    "            )\n",
    "\n",
    "        elif self.optimizer_type == \"AdamW\":\n",
    "            optimizer = optim.AdamW(\n",
    "                self.model.parameters(),\n",
    "                lr=self.learning_rate,\n",
    "                betas=(0.9, 0.999),\n",
    "                eps=1e-08,\n",
    "                weight_decay=0.0,\n",
    "                amsgrad=True,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        scheduler = {\n",
    "            'scheduler': LambdaLR(optimizer, self.lr_lambda),\n",
    "            'interval': 'step',\n",
    "            'frequency': 1,\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_lambda(step, warm_up_steps: int, reduce_lr_steps: int):\n",
    "    r\"\"\"Get lr_lambda for LambdaLR. E.g.,\n",
    "\n",
    "    .. code-block: python\n",
    "        lr_lambda = lambda step: get_lr_lambda(step, warm_up_steps=1000, reduce_lr_steps=10000)\n",
    "\n",
    "        from torch.optim.lr_scheduler import LambdaLR\n",
    "        LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    Args:\n",
    "        warm_up_steps: int, steps for warm up\n",
    "        reduce_lr_steps: int, reduce learning rate by 0.9 every #reduce_lr_steps steps\n",
    "\n",
    "    Returns:\n",
    "        learning rate: float\n",
    "    \"\"\"\n",
    "    if step <= warm_up_steps:\n",
    "        return step / warm_up_steps\n",
    "    else:\n",
    "        return 0.9 ** (step // reduce_lr_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(workspace, gpus, config_yaml, filename) -> NoReturn:\n",
    "    r\"\"\"Train & evaluate and save checkpoints.\n",
    "\n",
    "    Args:\n",
    "        workspace: str, directory of workspace\n",
    "        gpus: int\n",
    "        config_yaml: str, path of config file for training\n",
    "    \"\"\"\n",
    "\n",
    "    # # arugments & parameters\n",
    "    # workspace = args.workspace\n",
    "    # gpus = args.gpus\n",
    "    # config_yaml = args.config_yaml\n",
    "    # filename = args.filename\n",
    "\n",
    "    num_workers = 8\n",
    "    distributed = True if gpus > 1 else False\n",
    "    evaluate_device = \"cuda\" if gpus > 0 else \"cpu\"\n",
    "\n",
    "    # Read config file.\n",
    "    configs = read_yaml(config_yaml)\n",
    "    check_configs_gramma(configs)\n",
    "    task_name = configs['task_name']\n",
    "    input_source_types = configs['train']['input_source_types']\n",
    "    target_source_types = configs['train']['target_source_types']\n",
    "    input_channels = configs['train']['input_channels']\n",
    "    output_channels = configs['train']['output_channels']\n",
    "    batch_data_preprocessor_type = configs['train']['batch_data_preprocessor']\n",
    "    model_type = configs['train']['model_type']\n",
    "    loss_type = configs['train']['loss_type']\n",
    "    optimizer_type = configs['train']['optimizer_type']\n",
    "    learning_rate = float(configs['train']['learning_rate'])\n",
    "    precision = configs['train']['precision']\n",
    "    early_stop_steps = configs['train']['early_stop_steps']\n",
    "    warm_up_steps = configs['train']['warm_up_steps']\n",
    "    reduce_lr_steps = configs['train']['reduce_lr_steps']\n",
    "    resume_checkpoint_path = configs['train']['resume_checkpoint_path']\n",
    "\n",
    "    target_sources_num = len(target_source_types)\n",
    "\n",
    "    # paths\n",
    "    checkpoints_dir, logs_dir, logger, statistics_path = get_dirs(\n",
    "        workspace, task_name, filename, config_yaml, gpus\n",
    "    )\n",
    "\n",
    "    # training data module\n",
    "    data_module = get_data_module(\n",
    "        workspace=workspace,\n",
    "        config_yaml=config_yaml,\n",
    "        num_workers=num_workers,\n",
    "        distributed=distributed,\n",
    "    )\n",
    "\n",
    "    # batch data preprocessor\n",
    "    BatchDataPreprocessor = get_batch_data_preprocessor_class(\n",
    "        batch_data_preprocessor_type=batch_data_preprocessor_type\n",
    "    )\n",
    "\n",
    "    batch_data_preprocessor = BatchDataPreprocessor(\n",
    "        input_source_types=input_source_types, target_source_types=target_source_types\n",
    "    )\n",
    "\n",
    "    # model\n",
    "    # print(\"model_type\", model_type)\n",
    "    # return\n",
    "    Model = get_model_class(model_type=model_type)\n",
    "    model = Model(\n",
    "        input_channels=input_channels,\n",
    "        output_channels=output_channels,\n",
    "        target_sources_num=target_sources_num,\n",
    "    )\n",
    "\n",
    "    if resume_checkpoint_path:\n",
    "        checkpoint = torch.load(resume_checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        logging.info(\n",
    "            \"Load pretrained checkpoint from {}\".format(resume_checkpoint_path)\n",
    "        )\n",
    "\n",
    "    # loss function\n",
    "    loss_function = get_loss_function(loss_type=loss_type)\n",
    "\n",
    "    # callbacks\n",
    "    callbacks = get_callbacks(\n",
    "        task_name=task_name,\n",
    "        config_yaml=config_yaml,\n",
    "        workspace=workspace,\n",
    "        checkpoints_dir=checkpoints_dir,\n",
    "        statistics_path=statistics_path,\n",
    "        logger=logger,\n",
    "        model=model,\n",
    "        evaluate_device=evaluate_device,\n",
    "    )\n",
    "    # callbacks = []\n",
    "\n",
    "    # learning rate reduce function\n",
    "    lr_lambda = partial(\n",
    "        get_lr_lambda, warm_up_steps=warm_up_steps, reduce_lr_steps=reduce_lr_steps\n",
    "    )\n",
    "\n",
    "    # pytorch-lightning model\n",
    "    pl_model = LitSourceSeparation(\n",
    "        batch_data_preprocessor=batch_data_preprocessor,\n",
    "        model=model,\n",
    "        optimizer_type=optimizer_type,\n",
    "        loss_function=loss_function,\n",
    "        learning_rate=learning_rate,\n",
    "        lr_lambda=lr_lambda,\n",
    "    )\n",
    "\n",
    "    # trainer\n",
    "    trainer = pl.Trainer(\n",
    "        checkpoint_callback=False,\n",
    "        gpus=gpus,\n",
    "        callbacks=callbacks,\n",
    "        max_steps=early_stop_steps,\n",
    "        accelerator=\"ddp\",\n",
    "        sync_batchnorm=True,\n",
    "        precision=precision,\n",
    "        replace_sampler_ddp=False,\n",
    "        plugins=[DDPPlugin(find_unused_parameters=False)],\n",
    "        profiler='simple',\n",
    "        limit_train_batches=3,\n",
    "        limit_val_batches=3,\n",
    "    )\n",
    "\n",
    "    # Fit, evaluate, and save checkpoints.\n",
    "    trainer.fit(pl_model, data_module)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     parser = argparse.ArgumentParser(description=\"\")\n",
    "#     subparsers = parser.add_subparsers(dest=\"mode\")\n",
    "\n",
    "#     parser_train = subparsers.add_parser(\"train\")\n",
    "#     parser_train.add_argument(\n",
    "#         \"--workspace\", type=str, required=True, help=\"Directory of workspace.\"\n",
    "#     )\n",
    "#     parser_train.add_argument(\"--gpus\", type=int, required=True)\n",
    "#     parser_train.add_argument(\n",
    "#         \"--config_yaml\",\n",
    "#         type=str,\n",
    "#         required=True,\n",
    "#         help=\"Path of config file for training.\",\n",
    "#     )\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "#     args.filename = pathlib.Path(__file__).stem\n",
    "\n",
    "#     if args.mode == \"train\":\n",
    "#         train(args)\n",
    "\n",
    "#     else:\n",
    "#         raise Exception(\"Error argument!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accompaniment: 225752\n",
      "vocals: 225752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "lightning   : INFO     GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "lightning   : INFO     TPU available: None, using: 0 TPU cores\n",
      "pytorch_lightning.accelerators.gpu: INFO     LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "lightning   : INFO     initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "\n",
      "  | Name                    | Type                               | Params\n",
      "-------------------------------------------------------------------------------\n",
      "0 | batch_data_preprocessor | MixtureTargetBatchDataPreprocessor | 0     \n",
      "1 | model                   | ResUNet143_Subbandtime             | 103 M \n",
      "-------------------------------------------------------------------------------\n",
      "102 M     Trainable params\n",
      "787 K     Non-trainable params\n",
      "103 M     Total params\n",
      "413.442   Total estimated model params size (MB)\n",
      "lightning   : INFO     \n",
      "  | Name                    | Type                               | Params\n",
      "-------------------------------------------------------------------------------\n",
      "0 | batch_data_preprocessor | MixtureTargetBatchDataPreprocessor | 0     \n",
      "1 | model                   | ResUNet143_Subbandtime             | 103 M \n",
      "-------------------------------------------------------------------------------\n",
      "102 M     Trainable params\n",
      "787 K     Non-trainable params\n",
      "103 M     Total params\n",
      "413.442   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/3 [00:03<?, ?it/s] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  5.4644         \t|  100 %          \t|\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  3.5489         \t|1              \t|  3.5489         \t|  64.945         \t|\n",
      "get_train_batch                    \t|  1.7855         \t|1              \t|  1.7855         \t|  32.676         \t|\n",
      "run_training_batch                 \t|  1.763          \t|1              \t|  1.763          \t|  32.264         \t|\n",
      "optimizer_step_and_closure_0       \t|  1.7601         \t|1              \t|  1.7601         \t|  32.21          \t|\n",
      "training_step_and_backward         \t|  1.7589         \t|1              \t|  1.7589         \t|  32.189         \t|\n",
      "model_forward                      \t|  1.7589         \t|1              \t|  1.7589         \t|  32.188         \t|\n",
      "training_step                      \t|  1.7585         \t|1              \t|  1.7585         \t|  32.182         \t|\n",
      "on_train_end                       \t|  0.25607        \t|1              \t|  0.25607        \t|  4.6862         \t|\n",
      "on_train_start                     \t|  0.0015245      \t|1              \t|  0.0015245      \t|  0.027899       \t|\n",
      "on_epoch_start                     \t|  0.00074668     \t|1              \t|  0.00074668     \t|  0.013664       \t|\n",
      "on_batch_start                     \t|  0.00047327     \t|1              \t|  0.00047327     \t|  0.0086608      \t|\n",
      "on_train_batch_start               \t|  0.00024722     \t|1              \t|  0.00024722     \t|  0.0045241      \t|\n",
      "cache_result                       \t|  1.257e-05      \t|5              \t|  6.2852e-05     \t|  0.0011502      \t|\n",
      "on_train_epoch_start               \t|  1.8516e-05     \t|1              \t|  1.8516e-05     \t|  0.00033884     \t|\n",
      "on_fit_start                       \t|  1.8126e-05     \t|1              \t|  1.8126e-05     \t|  0.00033171     \t|\n",
      "on_before_accelerator_backend_setup\t|  7.945e-06      \t|1              \t|  7.945e-06      \t|  0.00014539     \t|\n",
      "\n",
      "lightning   : INFO     \n",
      "\n",
      "Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  5.4644         \t|  100 %          \t|\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  3.5489         \t|1              \t|  3.5489         \t|  64.945         \t|\n",
      "get_train_batch                    \t|  1.7855         \t|1              \t|  1.7855         \t|  32.676         \t|\n",
      "run_training_batch                 \t|  1.763          \t|1              \t|  1.763          \t|  32.264         \t|\n",
      "optimizer_step_and_closure_0       \t|  1.7601         \t|1              \t|  1.7601         \t|  32.21          \t|\n",
      "training_step_and_backward         \t|  1.7589         \t|1              \t|  1.7589         \t|  32.189         \t|\n",
      "model_forward                      \t|  1.7589         \t|1              \t|  1.7589         \t|  32.188         \t|\n",
      "training_step                      \t|  1.7585         \t|1              \t|  1.7585         \t|  32.182         \t|\n",
      "on_train_end                       \t|  0.25607        \t|1              \t|  0.25607        \t|  4.6862         \t|\n",
      "on_train_start                     \t|  0.0015245      \t|1              \t|  0.0015245      \t|  0.027899       \t|\n",
      "on_epoch_start                     \t|  0.00074668     \t|1              \t|  0.00074668     \t|  0.013664       \t|\n",
      "on_batch_start                     \t|  0.00047327     \t|1              \t|  0.00047327     \t|  0.0086608      \t|\n",
      "on_train_batch_start               \t|  0.00024722     \t|1              \t|  0.00024722     \t|  0.0045241      \t|\n",
      "cache_result                       \t|  1.257e-05      \t|5              \t|  6.2852e-05     \t|  0.0011502      \t|\n",
      "on_train_epoch_start               \t|  1.8516e-05     \t|1              \t|  1.8516e-05     \t|  0.00033884     \t|\n",
      "on_fit_start                       \t|  1.8126e-05     \t|1              \t|  1.8126e-05     \t|  0.00033171     \t|\n",
      "on_before_accelerator_backend_setup\t|  7.945e-06      \t|1              \t|  7.945e-06      \t|  0.00014539     \t|\n",
      "\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacity of 3.71 GiB of which 83.88 MiB is free. Including non-PyTorch memory, this process has 3.61 GiB memory in use. Of the allocated memory 3.30 GiB is allocated by PyTorch, and 94.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m config_yaml\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../scripts/4_train/musdb18/configs/vocals-accompaniment,resunet_subbandtime.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m filename \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(os\u001b[38;5;241m.\u001b[39mgetcwd())\u001b[38;5;241m.\u001b[39mstem\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_yaml\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_yaml\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 128\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(workspace, gpus, config_yaml, filename)\u001b[0m\n\u001b[1;32m    112\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m    113\u001b[0m     checkpoint_callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    114\u001b[0m     gpus\u001b[38;5;241m=\u001b[39mgpus,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m     limit_val_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    125\u001b[0m )\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Fit, evaluate, and save checkpoints.\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpl_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:513\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_dispatch()\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# dispath `start_training` or `start_testing` or `start_predicting`\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_dispatch()\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:553\u001b[0m, in \u001b[0;36mTrainer.dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mstart_predicting(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/pytorch_lightning/accelerators/accelerator.py:74\u001b[0m, in \u001b[0;36mAccelerator.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_training\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer):\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_type_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:111\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_training\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrainer\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# double dispatch to initiate the training loop\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:644\u001b[0m, in \u001b[0;36mTrainer.run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loop\u001b[38;5;241m.\u001b[39mon_train_epoch_start(epoch)\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;66;03m# run train epoch\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_training_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_steps \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_steps \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_step:\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/training_loop.py:492\u001b[0m, in \u001b[0;36mTrainLoop.run_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# ------------------------------------\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;66;03m# TRAINING_STEP + TRAINING_STEP_END\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# ------------------------------------\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 492\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_training_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# when returning -1 from train_step, we end epoch early\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_output\u001b[38;5;241m.\u001b[39msignal \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/training_loop.py:650\u001b[0m, in \u001b[0;36mTrainLoop.run_training_batch\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;66;03m# optimizer step\u001b[39;00m\n\u001b[0;32m--> 650\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_curr_step_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(\n\u001b[1;32m    654\u001b[0m         split_batch, batch_idx, opt_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhiddens\n\u001b[1;32m    655\u001b[0m     )\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/training_loop.py:426\u001b[0m, in \u001b[0;36mTrainLoop.optimizer_step\u001b[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    423\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m LightningOptimizer\u001b[38;5;241m.\u001b[39m_to_lightning_optimizer(optimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer, opt_idx)\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m \u001b[43mmodel_ref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_tpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_device_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDeviceType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTPU\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_TPU_AVAILABLE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_native_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_native_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_lbfgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_lbfgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/pytorch_lightning/core/lightning.py:1384\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(optimizer, LightningOptimizer):\n\u001b[1;32m   1382\u001b[0m     \u001b[38;5;66;03m# wraps into LightingOptimizer only for running step\u001b[39;00m\n\u001b[1;32m   1383\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m LightningOptimizer\u001b[38;5;241m.\u001b[39m_to_lightning_optimizer(optimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer, optimizer_idx)\n\u001b[0;32m-> 1384\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:219\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, *args, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen closure is provided, it should be a function\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    217\u001b[0m     profiler_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_step_and_closure_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 219\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofiler_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_optimizer_step_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:135\u001b[0m, in \u001b[0;36mLightningOptimizer.__optimizer_step\u001b[0;34m(self, closure, profiler_name, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m model \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mlightning_module\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(profiler_name):\n\u001b[0;32m--> 135\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_closure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39mtrain_loop\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    138\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain_loop\u001b[38;5;241m.\u001b[39mon_before_zero_grad(optimizer)\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/pytorch_lightning/accelerators/accelerator.py:278\u001b[0m, in \u001b[0;36mAccelerator.optimizer_step\u001b[0;34m(self, optimizer, opt_idx, lambda_closure, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m make_optimizer_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpre_optimizer_step(\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, optimizer, opt_idx, lambda_closure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    276\u001b[0m )\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m make_optimizer_step:\n\u001b[0;32m--> 278\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_closure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpost_optimizer_step(optimizer, opt_idx)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_type_plugin\u001b[38;5;241m.\u001b[39mpost_optimizer_step(optimizer, opt_idx, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/pytorch_lightning/accelerators/accelerator.py:283\u001b[0m, in \u001b[0;36mAccelerator.run_optimizer_step\u001b[0;34m(self, optimizer, optimizer_idx, lambda_closure, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_optimizer_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer: Optimizer, optimizer_idx: \u001b[38;5;28mint\u001b[39m, lambda_closure: Callable, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_type_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_closure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_closure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:160\u001b[0m, in \u001b[0;36mTrainingTypePlugin.optimizer_step\u001b[0;34m(self, optimizer, lambda_closure, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer, lambda_closure: Callable, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 160\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_closure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:130\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    129\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/torch/optim/adam.py:205\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 205\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    208\u001b[0m     params_with_grad: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/training_loop.py:644\u001b[0m, in \u001b[0;36mTrainLoop.run_training_batch.<locals>.train_step_and_backward_closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step_and_backward_closure\u001b[39m():\n\u001b[0;32m--> 644\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step_and_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhiddens\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/training_loop.py:738\u001b[0m, in \u001b[0;36mTrainLoop.training_step_and_backward\u001b[0;34m(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;124;03mwrap the forward step in a closure so second order methods work\u001b[39;00m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step_and_backward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;66;03m# lightning module hook\u001b[39;00m\n\u001b[0;32m--> 738\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhiddens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_curr_step_result \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m    741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip_backward \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtrain_loop\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    742\u001b[0m         \u001b[38;5;66;03m# backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/training_loop.py:293\u001b[0m, in \u001b[0;36mTrainLoop.training_step\u001b[0;34m(self, split_batch, batch_idx, opt_idx, hiddens)\u001b[0m\n\u001b[1;32m    291\u001b[0m model_ref\u001b[38;5;241m.\u001b[39m_results \u001b[38;5;241m=\u001b[39m Result()\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     training_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mpost_training_step()\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mlogger_connector\u001b[38;5;241m.\u001b[39mcache_logged_metrics()\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/pytorch_lightning/accelerators/accelerator.py:157\u001b[0m, in \u001b[0;36mAccelerator.training_step\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    154\u001b[0m args[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_device(args[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mtrain_step_context(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_type_plugin\u001b[38;5;241m.\u001b[39mtrain_step_context():\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_type_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/pytorch_lightning/plugins/training_type/ddp.py:287\u001b[0m, in \u001b[0;36mDDPPlugin.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py:1636\u001b[0m, in \u001b[0;36mDistributedDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1631\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistributedDataParallel.forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1632\u001b[0m     inputs, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_forward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1633\u001b[0m     output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1635\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_delay_all_reduce_all_params\n\u001b[0;32m-> 1636\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_ddp_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1637\u001b[0m     )\n\u001b[1;32m   1638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_forward(output)\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/torch/nn/parallel/distributed.py:1454\u001b[0m, in \u001b[0;36mDistributedDataParallel._run_ddp_forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1453\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inside_ddp_forward():\n\u001b[0;32m-> 1454\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/pytorch_lightning/overrides/base.py:48\u001b[0m, in \u001b[0;36m_LightningModuleWrapperBase.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m running_stage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mrunning_stage\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m running_stage \u001b[38;5;241m==\u001b[39m RunningStage\u001b[38;5;241m.\u001b[39mTRAINING:\n\u001b[0;32m---> 48\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# In manual_optimization, we need to prevent DDP reducer as\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# it is done manually in ``LightningModule.manual_backward``\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# `require_backward_grad_sync` will be reset in the\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# ddp_plugin ``post_training_step`` hook\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n",
      "Cell \u001b[0;32mIn[25], line 97\u001b[0m, in \u001b[0;36mLitSourceSeparation.training_step\u001b[0;34m(self, batch_data_dict, batch_idx)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Forward.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 97\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# output_dict: {\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m#     'waveform': (batch_size, target_sources_num * channels_num, segment_samples),\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# }\u001b[39;00m\n\u001b[1;32m    102\u001b[0m outputs \u001b[38;5;241m=\u001b[39m output_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwaveform\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 548\u001b[0m, in \u001b[0;36mResUNet143_Subbandtime.forward\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m    544\u001b[0m x \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m : x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# (bs, input_channels, T, F)\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# x: (batch_size, input_channels * subbands_num, padded_time_steps, freq_bins)\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m# UNet\u001b[39;00m\n\u001b[0;32m--> 548\u001b[0m x1_pool, x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_block1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# x1_pool: (bs, 32, T / 2, F / 2)\u001b[39;00m\n\u001b[1;32m    549\u001b[0m x2_pool, x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_block2(x1_pool)  \u001b[38;5;66;03m# x2_pool: (bs, 64, T / 4, F / 4)\u001b[39;00m\n\u001b[1;32m    550\u001b[0m x3_pool, x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_block3(x2_pool)  \u001b[38;5;66;03m# x3_pool: (bs, 128, T / 8, F / 8)\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 132\u001b[0m, in \u001b[0;36mEncoderBlockRes4B.forward\u001b[0;34m(self, input_tensor)\u001b[0m\n\u001b[1;32m    130\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_block2(encoder)\n\u001b[1;32m    131\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_block3(encoder)\n\u001b[0;32m--> 132\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_block4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m encoder_pool \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mavg_pool2d(encoder, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoder_pool, encoder\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 84\u001b[0m, in \u001b[0;36mConvBlockRes.forward\u001b[0;34m(self, input_tensor)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Forward data into the module.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m    output_tensor: (batch_size, output_feature_maps, time_steps, freq_bins)\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(F\u001b[38;5;241m.\u001b[39mleaky_relu_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(input_tensor), negative_slope\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m))\n\u001b[0;32m---> 84\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleaky_relu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_slope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_shortcut:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshortcut(input_tensor) \u001b[38;5;241m+\u001b[39m x\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/music_source_separation/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacity of 3.71 GiB of which 83.88 MiB is free. Including non-PyTorch memory, this process has 3.61 GiB memory in use. Of the allocated memory 3.30 GiB is allocated by PyTorch, and 94.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import os\n",
    "\n",
    "workspace = \".\"\n",
    "gpus=1\n",
    "config_yaml=\"../scripts/4_train/musdb18/configs/vocals-accompaniment,resunet_subbandtime.yaml\"\n",
    "filename = pathlib.Path(os.getcwd()).stem\n",
    "train(\n",
    "    workspace=workspace,\n",
    "    gpus=gpus,\n",
    "    config_yaml=config_yaml,\n",
    "    filename=filename\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
